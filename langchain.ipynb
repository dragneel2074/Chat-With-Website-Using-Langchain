{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TDP6d00RjZ0l",
        "outputId": "cedc9ada-845d-4622-8d36-6bb7305b6b0f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.0.342)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.23)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.8.6)\n",
            "Requirement already satisfied: anyio<4.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.7.1)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.6.3)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.33)\n",
            "Requirement already satisfied: langchain-core<0.1,>=0.0.7 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.0.7)\n",
            "Requirement already satisfied: langsmith<0.1.0,>=0.0.63 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.0.67)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.23.5)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.10.13)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (3.3.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (3.4)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (1.1.3)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.20.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.5.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2023.7.22)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.1)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.7,>=0.5.7->langchain) (23.2)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.10/dist-packages (1.7.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain\n",
        "!pip install faiss-cpu\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "urls = [\n",
        "    \"https://kickerai.com/exploring-sentiment-analysis-with-nepali-text/\",\n",
        "\"https://kickerai.com/how-to-get-started-on-flutter-for-both-non-programmers-and-programmers/\",\n",
        "\"https://kickerai.com/swiftui-building-a-todo-app/\",\n",
        "\"https://kickerai.com/3d-blender-step-by-step-tutorial-how-to-create-a-simple-vase-in-blender/\",\n",
        "\"https://kickerai.com/git-worktrees-why-would-you-use-it/\",\n",
        "\"https://kickerai.com/flutter-how-to-hide-keyboard-on-iphone-android/\",\n",
        "\"https://kickerai.com/getting-started-with-blender-introduction/\",\n",
        "\"https://kickerai.com/creating-a-qrcode-scanner-in-ios-in-swift-swift-tutorial/\",\n",
        "\"https://kickerai.com/login-and-register-easily-with-flutter-using-firebase/\",\n",
        "\"https://kickerai.com/getting-started-with-blender-things-to-avoid-when-starting-out/\",\n",
        "\"https://kickerai.com/blender-3d-tutorial-understanding-spline-modeling-in-blender/\",\n",
        "\"https://kickerai.com/how-to-add-lighting-in-blender-easy-blender-3d-tutorial/\",\n",
        "\"https://kickerai.com/how-to-create-a-dialog-box-in-flutter-dialogbox-with-radio-button/\",\n",
        "\"https://kickerai.com/add-lighting-in-blender-part-2/\",\n",
        "\"https://kickerai.com/create-a-simple-dialog-box-in-flutter/\",\n",
        "\"https://kickerai.com/flutter-rounded-container-code-example/\",\n",
        "\"https://kickerai.com/flutter-errors-and-solution/\",\n",
        "\"https://kickerai.com/internet-connection-checker-using-bloc-flutter-code-example/\",\n",
        "\"https://kickerai.com/face-recognition-attendance-system/\",\n",
        "\"https://kickerai.com/unwrap-key-failed-flutter-error/\",\n",
        "\"https://kickerai.com/how-to-fetch-data-from-internet-flutter-tutorial/\",\n",
        "\"https://kickerai.com/why-isnt-pubsec-in-dart/\",\n",
        "\"https://kickerai.com/face-liveness-detection-via-opencv-and-tensorflow/\",\n",
        "\"https://kickerai.com/solved-dont-use-one-refreshcontroller-to-multiple-smartrefresher/\",\n",
        "\"https://kickerai.com/create-games-using-flutter-tic-tac-toe/\",\n",
        "\"https://kickerai.com/flutter-error-the-android-gradle-plugin-supports-only-kotlin/\"\n",
        "]"
      ],
      "metadata": {
        "id": "0EaTMqGFj20z"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n"
      ],
      "metadata": {
        "id": "70KcUBjLkzjo"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AnUyC6n5lGVf"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install unstructured"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wep7QAhPmHz5",
        "outputId": "ea80d025-1786-4060-dc33-38a7c8b866f9"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: unstructured in /usr/local/lib/python3.10/dist-packages (0.11.0)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.10/dist-packages (from unstructured) (5.2.0)\n",
            "Requirement already satisfied: filetype in /usr/local/lib/python3.10/dist-packages (from unstructured) (1.2.0)\n",
            "Requirement already satisfied: python-magic in /usr/local/lib/python3.10/dist-packages (from unstructured) (0.4.27)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from unstructured) (4.9.3)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from unstructured) (3.8.1)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from unstructured) (0.9.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from unstructured) (2.31.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from unstructured) (4.11.2)\n",
            "Requirement already satisfied: emoji in /usr/local/lib/python3.10/dist-packages (from unstructured) (2.8.0)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.10/dist-packages (from unstructured) (0.6.3)\n",
            "Requirement already satisfied: python-iso639 in /usr/local/lib/python3.10/dist-packages (from unstructured) (2023.6.15)\n",
            "Requirement already satisfied: langdetect in /usr/local/lib/python3.10/dist-packages (from unstructured) (1.0.9)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from unstructured) (1.23.5)\n",
            "Requirement already satisfied: rapidfuzz in /usr/local/lib/python3.10/dist-packages (from unstructured) (3.5.2)\n",
            "Requirement already satisfied: backoff in /usr/local/lib/python3.10/dist-packages (from unstructured) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from unstructured) (4.5.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from unstructured) (1.14.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->unstructured) (2.5)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json->unstructured) (3.20.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json->unstructured) (0.9.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from langdetect->unstructured) (1.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured) (4.66.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->unstructured) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->unstructured) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->unstructured) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->unstructured) (2023.7.22)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->unstructured) (23.2)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json->unstructured) (1.0.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import UnstructuredURLLoader\n",
        "loaders = UnstructuredURLLoader(urls=urls)\n",
        "data = loaders.load()"
      ],
      "metadata": {
        "id": "rGOR_lcclayq"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BBNLxv48lsvj",
        "outputId": "f612573c-60da-4c1b-e0a3-ccaaa4db1ffb"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "26"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KOdI2Sbumem1",
        "outputId": "ffaf912a-2865-4dae-c4bd-d25e586623f8"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content=\"Exploring Sentiment Analysis with Nepali Text\\n\\nOctober 30, 2023\\n\\nby Prasant\\n\\nSentiment analysis, a crucial component of natural language processing (NLP), has seen exponential growth in its applications, spanning from understanding customer feedback to social media monitoring. In this blog, I delve into a project that tackled sentiment analysis of Nepali text, illustrating the process, challenges, and achievements along the way.\\n\\nshow\\n\\n1\\n                Background\\n\\n2\\n                Data and Preprocessing\\n\\n2.1\\n                Loading the Data\\n\\n2.2\\n                Cleaning\\n\\n2.3\\n                Text Preprocessing\\n\\n3\\n                Feature Engineering with TF-IDF\\n\\n4\\n                Model Selection and Parameter Tuning\\n\\n4.1\\n                Parameters for GridSearchCV:\\n\\n4.2\\n                Parameters of the estimator (SVC in our Case)\\n\\n4.3\\n                Process\\n\\n5\\n                Results\\n\\n6\\n                Creating A Web Application\\n\\n7\\n                What More Can Be Done for Better Accuracy?\\n\\n7.1\\n                Conclusion\\n\\nBackground\\n\\nNepali, an Indo-Aryan language, is spoken predominantly in Nepal and parts of India. Despite its wide usage, resources for NLP tasks in Nepali are limited compared to languages like English. This project aimed to fill this gap by creating a sentiment analysis model that can accurately classify Nepali sentences into positive, negative, or neutral sentiments.\\n\\nData and Preprocessing\\n\\nThe journey began with the dataset, sentiment_analysis_nepali_final.csv, which contained three columns: Unnamed: 0, Sentences, and Sentiment. Each row represented a Nepali sentence and its associated sentiment. The initial preprocessing steps included:\\n\\nLoading the Data\\n\\ndata = pd.read_csv('/work/sentiment_analysis_nepali_final.csv')\\n\\nCleaning\\n\\nDropping unnecessary columns and renaming for clarity.\\n\\nHandling null values and duplicates.\\n\\nText Preprocessing\\n\\nRemoval of stopwords in Nepali using the nltk library.\\n\\nTokenization and normalization of the text.\\n\\nFeature Engineering with TF-IDF\\n\\nThe next step was to convert the preprocessed text into numerical features that the model could understand. This was done using Term Frequency-Inverse Document Frequency (TF-IDF), a technique that reflects the importance of words in the corpus.\\n\\nModel Selection and Parameter Tuning\\n\\nI chose the Support Vector Machine (SVM) for this task due to its effectiveness in classification problems, especially with high-dimensional data. However, SVM has crucial hyperparameters like C, kernel, and gamma that needed optimization for best performance.\\n\\nParameters for GridSearchCV:\\n\\nestimator:\\n\\nIn our case, svm.SVC(). This is the model for which we want to find the best hyperparameters.\\n\\nparam_grid:\\n\\nA dictionary specifying the hyperparameters and their possible values. For example, parameters = {'C': [0.01, 0.1, 1, 10, 100], 'kernel': ['rbf'], 'gamma': ['scale', 'auto']}.\\n\\ncv:\\n\\nStands for cross-validation. cv=5 means you’re using 5-fold cross-validation. This means the data is divided into 5 parts; in each iteration, 4 parts are used for training and 1 for validation.\\n\\nscoring:\\n\\nThe metric to evaluate the models. In our case, we are using 'accuracy', so the grid search will select the hyperparameters that achieve the highest accuracy.\\n\\nverbose:\\n\\nControls the verbosity of the output. verbose=2 means we will get more detailed output (such as the progress of the fits).\\n\\nn_jobs:\\n\\nSpecifies the number of jobs to run in parallel. n_jobs=-1 means using all processors.\\n\\nParameters of the estimator (SVC in our Case)\\n\\nC:\\n\\nRegularization parameter. The strength of the regularization is inversely proportional to C. Lower values of C lead to stronger regularization(prevent overfitting). Helps to control over-fitting (Overfitting occurs when a model learns the details and noise in the training data to the extent that it negatively impacts the model’s performance).\\n\\nkernel:\\n\\nSpecifies the kernel type to be used in the algorithm. we are using 'rbf' which stands for Radial Basis Function, a common choice for non-linear data.\\n\\ngamma:\\n\\nA parameter for non-linear hyperplanes. The higher the gamma value, the more it tries to exactly fit the training data set. scale and auto are two ways of setting this parameter in scikit-learn.\\n\\nProcess\\n\\nThe GridSearchCV will iterate over all combinations of these parameters (5 values of C, 1 kernel type, and 2 values of gamma), applying 5-fold cross-validation for each combination.\\n\\nIt will measure the performance using accuracy and finally pick the combination that gives the best result.\\n\\nThis process helps in finding the optimal hyperparameters for the SVM model for your specific dataset.\\n\\nThis is where the bulk of the time was spent — approximately 6 hours — in finding the best parameters through GridSearchCV, a method that systematically works through multiple combinations of parameter tunes, cross-validating as it goes to determine which tune gives the best performance.\\n\\nResults\\n\\nThe best parameters were found to be {'C': 10, 'gamma': 'scale', 'kernel': 'rbf'}, achieving a cross-validation score of 0.67 and a test set score of 0.68. These results are promising but also indicate room for improvement.\\n\\nThen using some Nepali text, I tested the model. Surprisingly, the predictions were mostly accurate.\\n\\nCreating A Web Application\\n\\nThe next step was to create a web application using Streamlit. For this, I exported two files using Pickle.\\n\\nThen using those files, I created the app.\\n\\nYou can find the full code on github.\\n\\nThe dataset is from Kaggle and the article on the medium was helpful for this project.\\n\\nWhat More Can Be Done for Better Accuracy?\\n\\nEnhanced Data Preprocessing:\\n\\nMore advanced techniques for text normalization and dealing with colloquialisms in Nepali.\\n\\n\\n\\nAugmenting data through techniques like synonym replacement.\\n\\nExperimenting with Different Models:\\n\\nTrying out ensemble models or deep learning approaches like LSTM or Transformers.\\n\\nHyperparameter Tuning:\\n\\nFurther refinement of SVM parameters or exploring other algorithms and their optimal settings.\\n\\nIncorporating Contextual Information:\\n\\nUsing pre-trained models that understand the context better, like BERT-based models for Nepali.\\n\\nError Analysis:\\n\\nDiving deeper into the misclassified examples to understand where the model is faltering and why.\\n\\nConclusion\\n\\nThis project showcased the intricacies of sentiment analysis, especially when dealing with a less-resourced language like Nepali. The journey from data preprocessing to hyperparameter tuning was arduous, consuming around 6 hours just for parameter optimization.\\n\\nYet, the results were promising, opening doors to more refined techniques and models that can push the accuracy even further. As the field of NLP grows, the horizon for languages like Nepali expands, bringing a more robust and nuanced understanding to the world of sentiment analysis.\\n\\nCategories NLP\\n\\nTags\\n\\nnepali language,\\n\\nsentiment analysis,\\n\\nsentiment analysis on nepali text,\\n\\nsentiment analysis using nepali language\\n\\n3D Blender Step-By-Step Tutorial: How to Create a Simple Vase in Blender?\", metadata={'source': 'https://kickerai.com/exploring-sentiment-analysis-with-nepali-text/'}),\n",
              " Document(page_content='How to Get Started on Flutter | For Both Non Programmers and Programmers\\n\\nAugust 20, 2023\\n\\nby Prasant\\n\\nFlutter Tutorial for Beginners\\n\\nAre you interested in learning Flutter? But you don’t know where to start or how to get started learning flutter and dart. Then look no further!\\n\\nIn this post, I will be discussing the best ways of learning flutter. It won’t be a problem if you are not from a programming background as long as you are willing to learn.\\n\\nTable of Contents:\\n\\nWhy Should You Learn Flutter\\n\\nHow to get started\\n\\nBest Paid Method\\n\\nBest Free Method\\n\\nFrequently Asked Questions\\n\\nFirst of all, Let’s discuss why should you learn Flutter, not other programming languages.\\n\\nTo explain in few words, with a single codebase you write in Flutter, you can build applications for all three- Android, IOS, and Web.\\n\\nFlutter is Google’s UI toolkit for building beautiful, natively compiled applications for mobile, web, and desktop from a single codebase.\\n\\nflutter.dev\\n\\nFlutter is easy to learn as well compared to other programming languages.\\n\\nFlutter]\\n\\nSome Other reasons are:\\n\\nYou can create beautiful Apps using Flutter. Designing is easy.\\n\\nReal time compilation. You can change code anytime and easily spot which code is causing problems.\\n\\nYou can save time and money as you don’t need to code differently for IOS or Android.\\n\\nFlutter has a very good documentation, full of rich knowledge about its tutorials, widgets, classes and many more.\\n\\nThese are some of the reasons why Flutter is so popular nowadays.\\n\\n[Note: Flutter is not a programming language, but is a Software Development Kit(SDK) based on Dart.]\\n\\nNow that you know the importance of learning Flutter. Let’s discuss how to get started.\\n\\nGetting started in any programming language is not easy. One should be devoted to learning. And the most important thing is to practice and make use of all the learnings. If not, you can easily find yourself stuck and losing motivation.\\n\\nThere are no secrets to success. It is the result of preparation, hard work, learning from failure.\\n\\nThe best way of Getting Started with Flutter.\\n\\nThe best way I personally believe is from the Udemy Flutter course The Complete 2021 Flutter Development Bootcamp with Dart by Dr. Angela Yu.\\n\\nWhy am I recommending this course?\\n\\nThis is a great course to learn if you want to get the most out of Flutter. You will be able to build a portfolio of beautiful Flutter apps you can add to your resume to impress recruiters and potential employers. It’s not a free course but it is completely worth your time and money if you are serious about learning Flutter. You can buy $10 on Udemy sales.\\n\\nWith this course, you will be able to learn many things about Flutter that every beginner should know like what is variable, class, objects, functions, and other necessary knowledge. So There is no need for beginners with a non-programming background to worry, all of the necessary knowledge is included in this course.\\n\\nIn this course, you will start from making a simple App to a chatting Application.\\n\\nThis course also guides you on how to install Flutter on your device for both Windows and Mac. What are the necessary steps needed to be done before you can make an Application! All of this is covered by this course.\\n\\nYou will start by learning how to make a simple Application and at the end, you will be able to make a chatting Application. Other Applications you will learn to make includes Dice App, Quiz App, and Weather App.\\n\\nThere are also various challenges given in the course to complete. Don’t worry if you can’t complete it, Angela will show you the solutions.\\n\\nIn short, this course offers:\\n\\nAll Necessary Basic Knowledge that every programmer should know.\\n\\nBasic introduction about Flutter\\n\\nHow to install Flutter on both Windows and Mac\\n\\nFrom making a simple App to Complex App like Weather and Chatting App\\n\\nWell Detailed explanation about every step\\n\\nThat’s why it is the best course out there to learn if you are serious about Flutter and want to make it a career out of it.\\n\\nLearn Flutter Course for Beginners on Udemy\\n\\nBesides this, there is a best Free method if you don’t want to pay: Youtube\\n\\nThis 5 hour course by Academind on Youtube covers all the important topics about Flutter for Beginners.\\n\\nBoth the courses I mentioned are best. It’s doesn’t matter which you choose to get started.\\n\\nTo Learn anything, Remember you just need to GET STARTED RIGHT NOW.\\n\\nFAQ\\n\\nHow do I get started with flutter?\\n\\n\\nThe best way of getting started with Flutter is from this Udemy Course “Flutter Development for Beginners” or “Flutter Crash Course for Beginners 2021” by Academind on Youtube.\\n\\nDoes Flutter requires Coding?\\n\\n\\nYes. You need to code to create a beautiful application for mobile and desktop.\\n\\nWhat should I learn before flutter?\\n\\n\\nIf you take this Udemy course “Flutter Development for Beginners” then you don’t need to learn anything. Most of the basic knowledge needed is covered by this course. It is the best course out there for a person with a non-programming or programming background.\\n\\nIs it worth learning Flutter in 2021?\\n\\n\\nYes. Flutter is getting popular day by day and is being used by big companies like Google, Tencent, Alibaba, eBay, etc\\n\\nWhich course is best for learning Flutter?\\n\\n\\nThe best way of getting started with Flutter is from this Udemy Course “Flutter Development for Beginners” or “Flutter Crash Course for Beginners 2021” by Academind on Youtube.\\n\\nCategories Flutter\\n\\nTags\\n\\nBeginners Flutter Learning,\\n\\ndart,\\n\\nflutter for beginners,\\n\\nhow to get started with flutter\\n\\nFlutter: How to Hide Keyboard on iPhone & Android\\n\\nExploring Sentiment Analysis with Nepali Text', metadata={'source': 'https://kickerai.com/how-to-get-started-on-flutter-for-both-non-programmers-and-programmers/'}),\n",
              " Document(page_content='SwiftUI: Building a Todo App\\n\\nAugust 20, 2023\\n\\nby Sajal Limbu\\n\\nSwiftUI is the hottest topic in iOS Development. It is a declarative UI pattern written in Swift for app development in Apple devices. It allows us to write apps with syntax declarations and support hot reloading or live preview and many other feature sets which makes it easier to develop apps or implement features for future updates.\\n\\nWhat is a Declarative UI?\\n\\n“Declarative UI” means you describe in some kind of language what elements you need in your UI, and to some degree how they should look like, but you can leave out details like the exact position and visual style of the elements. For example, in HTML you can describe that you want an input field, but how and where this field will be placed in the UI is highly dependent on the browser you are using.\\n\\nToday we’ll learn how we can build a simple Todo app for iOS.\\n\\nThings you’ll need\\n\\nA computer running macOS\\n\\nXcode\\n\\nPreferably, a physical iOS device or you can just use emulator\\n\\nThings you’ll learn\\n\\nCreating views using SwiftUI\\n\\nUsing SF Symbols for Images\\n\\nState management\\n\\nExtracting views for modularity\\n\\nSwipe to delete rows in a List view\\n\\nWhat you’ll build\\n\\nTable of Contents\\n\\nSwiftUI views used in the project\\n\\nView Modifiers\\n\\nStarting a new SwiftUI projectCreating an Apple IdSigning In with your Apple Id in Xcode\\n\\nCreating Text view\\n\\nCreating Data model class\\n\\nExtracting Sub-view\\n\\nCreating List view\\n\\nCreating Stacks\\n\\nCreating Button view\\n\\nFinal code\\n\\nBelow are the SwiftUI views that we’ll use in this project\\n\\nText(“Placeholder”) – As the name suggests, it adds a text view.\\xa0Eg: Text(“Namaste”)\\n\\nTextField(title: StringProtocol, text: Binding<String>) – It adds a text input field. The placeholder text for the view is defined by the value passed in StringProtocol while the input value will be bound to the Binding variable.Eg:\\n\\n@State var name: String = “”\\n\\nTextField(“Username”,text: $name)\\n\\nHere, we’ve used @State annotation which will be explained later in the article.\\n\\nSpacer() – It is an empty field that is used to add space between different views.\\n\\nButton(action:{},label:{}) – It is used to create a Button view. The action closure is called when certain actions are performed on the Button. The label closure describes the style of the Button label.\\n\\nStacks: There are 3 stack views in SwiftUI. The order on how the views inside will be displayed is always from top to bottom(for ZStack the top layer is always the bottom most layer and vice-versa).HStack {} – Stacks view horizontallyVStack {} – Stacks view verticallyZStack {} – Stacks view in the z-index i.e. on top of each other\\n\\nList {} – It is used to create a list of elements.\\n\\nImage() – It is used to add an image view.\\n\\nThere are two ways to add Views, either we can manually write the initialization code or we can click on the library icon, search the required view and then drag the view to where it is needed.\\n\\nThere are different types of modifiers that we can add to each view to customize the view according to our needs. This is one of the ways that SwiftUI makes building UI’s easier as the views can share properties between each other. Some of the most common modifiers are:\\n\\nforegroundColor(.blue) – Sets the color for any view of the UI in the foreground\\n\\nbackgroundColor(.blue) – Sets the color of any view of the UI in the background\\n\\npadding() – Adds a default padding for the view on all the edges( left, right, top, bottom ). We can also specify the edge or the amount of padding.\\n\\nshadow() – Adds shadow to the view. The type of shadow can be defined with color, color opacity, radius and the xy axis values.\\n\\nfont() – It is the most common modifier used for text related fields. We can assign a custom font family or font size to the text using this modifier.\\n\\nframe() – It is most commonly used for Images. It is used to give custom values to image fields. Note: To make any image customizable, we need to assign the resizable() modifier to the view. Only then can other modifiers be applied.\\n\\nLet’s start with creating a new Xcode project.\\n\\nOpen Xcode and select Start a new project. If it is already open, goto File > New > Project.\\n\\nSelect App under iOS tab and click Next.\\n\\nGive a product name of your choosing. I named it Todo.\\n\\nSelect a team, If you don’t see anything listed. Follow the steps below.\\n\\nCreate an apple id here\\n\\nAfter the apple id is created successfully, add your account to Xcode. Goto Xcode > Preferences > Accounts > Click on the + on the bottom left of the window.\\n\\nAfter logging in, close Xcode and open it again. On the project creation page, you should now see your name John Doe(Personal Team). For learning purposes, it is fine to use this account.\\n\\nOrganization identifier – Following convention, you can give it your name with a prefix(com.) and a suffix at the end. The whole reason for an identifier is to make sure that the app is unique. Using a personal developer account, you are only allotted a certain number of unique identifiers per week. If you are creating multiple apps using this account, you can use the same identifier(only do this for learning app development). Your identifier should look something like shown below:\\xa0com.johndoe.app\\n\\nSelect SwiftUI interface as we’re gonna be building a SwiftUI app.\\n\\nFor Life Cycle, currently you’ll find two options: SwiftUi app and UIKit App Delegate. Choose UIKit App Delegate for now.\\n\\nSelect language as Swift.\\n\\nUncheck ‘Use Core Data’ if it is checked. We’ll not be using any data persistence for now.\\n\\nChoose a folder to create your project at. I generally use a Workspace folder, where I store all my projects. Then, hit Create. If everything goes well, you should see the following structure on the left side of your Xcode window.\\n\\nXcode also automatically opens the ContentView.swift file that holds all your view information for now. If your canvas is active, you should see a live preview of how the app looks like. If not you can enable your canvas using the editor options.\\n\\nNow, we can finally begin to write some code.\\n\\nStep 1: In your ContentView.swift file, we are going to make some changes.\\n\\nFirst, replace the default text that says “Hello World” with “To do”\\n\\nText(\"To do\")\\n\\nNow, we can add a modifier to change the color of the text. We add modifiers to views using dot operators(.) . Add .foregroundColor() modifier to change the text color to blue. Here, .blue is the predefined system color.\\n\\nText(\"To do\").foregroundColor(.blue)\\n\\nNote: foregroundColor() modifier will work differently depending on the view.\\n\\nIf you press Command + B to build the app and your canvas is open and not paused, a preview of the change will be visible.\\n\\nStep 2: We now need a class that will be used to structure our data.\\n\\nCreate a new Swift file by right-clicking on your root project folder and selecting New File. Give it a name of Items and click create. This will be our Model.\\n\\nstruct Items {\\n\\nlet date = Date()\\n\\nlet data: String\\n\\nIn your Items.swift file, Create a new struct like so. Here, date is a computed field and it’s value will be initialized when an instance object from Items class is created. Whereas, data is a variable of type String and it’s value can be assigned upon initialization. Here both of the variables are constants denoted by let, which means once initialized the value is immutable i.e. it cannot be changed.\\n\\nWhat is a computed property?\\n\\nA computed property is a special feature which allows the value of the variable to be computed on the object initialization.\\n\\nstruct Ram {\\n    var food: String = \"Mango\"\\n    var likes: String {\\n        return \"Ram likes \\\\(food)\"\\n    }\\n}\\n\\n//Ram().likes returns \"Ram likes Mango\"\\n\\nStep 3: Add a TextField that’ll be used to input our data.\\n\\nFirst, we need to embed our Text view into a Vertical Stack or VStack. You can do this by creating a VStack { //sub-view } and adding your Text inside the VStack curly braces {}. But there’s an easy way to do this, Press Command and then left click the Text field.\\n\\nClick on the line where it says Embed to VStack. This will work the same as mentioned above.\\n\\nBefore adding a TextField, we need a place to store the input value. For this we need to initialize a variable of type string. We’ll add a @State annotation on it, to signify that it’s state is being monitored and any change of the state will be reflected on all of it’s instances. It basically means that a change in it’s value will change all the places where it is being used.\\n\\n@State var todoData:String = \"\"\\n\\nWe initialize the variable as empty. Note that empty is not equal to nil.\\n\\nAs to why we initialized it beforehand and not when we get the input data, we’ll need to talk about optionals which will be covered in a future article. We can now add the TextField view.\\n\\nTextField(\"Things to do\", text: $todoData)\\n\\n.textFieldStyle(RoundedBorderTextFieldStyle())\\n\\n.padding(10)\\n\\nI’ve used two modifiers to make it look nicer. The RoundedBorderTextFieldStyle() will create rounded edges border around the TextField view. While padding(), is a generic padding modifier with some value.\\n\\nStep 4: Creating the list view\\n\\nFirst, Create an array of Items where we’ll store our data. This will also be state monitored.\\n\\n@State var allItems = [Items]()\\n\\nSince, we’re going to be placing a floating button above the list. We need a ZStack to layer them on top of each other.\\n\\nAdd a ZStack view below the TextField view inside the VStack.\\n\\nNote: It’s helpful and easy to build UI if you keep the view hierarchy in mind, for our app we need:\\n\\nA root VStack for Text,TextField and List view.\\n\\nA ZStack to layer List view and Button view.\\n\\nAnother VStack to bring the Button view to the bottom.\\n\\nA HStack to set the Button position to the bottom right of the screen.\\n\\nCreate a List view inside the ZStack like so,\\n\\nList {\\n\\nForEach(self.allItems, id: \\\\.date) { item in\\n\\nText(item.data)\\n\\n.onDelete(perform: deleteItem)\\n\\nHere, We’re using the ForEach function to loop over the Items array and showing the data for each array element in a Text view. We also have a .onDelete modifier for the List view which will facilitate us the feature of swipe to delete. We’ll delegate this action to a separate function inside our ContentView.\\n\\nfunc deleteItem(at offSets: IndexSet) {\\n\\nallItems.remove(atOffsets: offSets)\\n\\nTo create a Button view, add the following code inside the action closure.\\n\\nHere, when the button is pressed\\n\\nWe check if the input value is empty, we should not add any values to the list if it is empty.\\n\\nIf it is not empty then we can create a new data object from Items class using the input data.\\n\\nWe then append the data to the global array of Items and reset the input field\\n\\nInstead of a text, we’ve used a SF Symbol image and added some modifiers to it\\n\\nWe’ve also added shadow modifiers to the parent Button view\\n\\nYou can find more info on SF Symbols here.\\n\\nYou can download a macOS app to see all the available images.\\n\\nWe’re also tapping into the shared application singleton to dismiss the keyboard after data has been added. Just copy and paste this for now.\\n\\nIf you run this app now, you’ll find that it is working fine but the Button view is not appearing correctly. We can fix this by adding a VStack and a spacer, then adding a HStack and a spacer after which we will add the newly created Button view.\\n\\nAfter this is complete, we now have a working SwiftUI application that stores data(not persistent), has a reactive view and supports swipe to delete.\\n\\nBut we can further refactor our app so that some functionalities can be delegated to different views or extensions to support modularity.\\n\\nWe can start by extracting our Button image sub-view. We can do this by holding\\n\\nCommand and left clicking the Image view and selecting ‘Extract Subview’.\\n\\nRight click on the ExtractedView and refactor it to ‘ButtonImage’ or a name appropriate to the view. This method of modularity helps us extract sub-views to their own individual view and use the same code in different parts of the project but we must initialize the values that are being passed to the view. In our case, we’re passing the image name. So, we create a let constant in our extracted view called imageName.\\n\\nWe can then pass the image name directly to the extracted view.\\n\\nButtonImage(imageName: \"plus.circle.fill\")\\n\\nWe can also extend our View to support custom functions using “extension”. We’ll use this feature to extend our root View to support hideKeyboard functionality.\\n\\nPaste the above code outside the ContentView. Then replace the code for keyboard hide with\\n\\nhideKeyboard()\\n\\nYou should have something like this after\\n\\nIf you followed everything correctly, your final ContentView.swift should look something like this\\n\\nAnd your Items.swift file should look like this\\n\\nYou can start adding data to the list, swipe-left once to show the delete options and swipe again to delete the row.\\n\\nThat’s it! You now have a working todo app with swipe gesture built using SwiftUI.\\n\\nMiscellaneous\\n\\nYou can select the preview device by selecting an emulator or changing the ContentView() in your PreviewProvider.\\n\\nIf you’re dealing with a large project. It is always best to categorize your files into group folders. You can group them together by selecting all the needed files and selecting New group from the selection on right-click.\\n\\nIf you ever want to change your default ContentView.swift file name. The safest way to do this is using the refactor command. Right-click on your ContentView and select Refactor > Rename. This will change all the places where this name is being used. If changing this breaks your live preview, change the struct ContentView_Previews to struct YourChangedName_Previews and resume the canvas again.\\n\\nCategories Blog\\n\\nTags swiftui, swiftui app todo, TODO app with swiftUI\\n\\nFlutter: How to Hide Keyboard on iPhone & Android\\n\\nExploring Sentiment Analysis with Nepali Text\\n\\nLeave a Comment\\n\\nComment', metadata={'source': 'https://kickerai.com/swiftui-building-a-todo-app/'}),\n",
              " Document(page_content='3D Blender Step-By-Step Tutorial: How to Create a Simple Vase in Blender?\\n\\nAugust 20, 2023\\n\\nby Prasant\\n\\nChapter 3: Want to make a simple vase?\\n\\nWelcome to the third chapter in the Blender Tutorial Series where we will learn to build a vase. Before you start reading, do check out other chapters as well.\\n\\nChapter 1 talks about the basics of the\\xa0Blender\\xa0and its\\xa0functionalities.\\n\\nChapter 1: Introduction to Blender\\n\\nChapter 2 talks about things that you should avoid as a beginner.\\n\\nChapter 2: Things to Avoid When Starting Out\\n\\nLet’s learn a quick technique to help you with understanding some basic hotkeys and tools we have. To make a vase in blender, follow these simple steps below:\\n\\nHold the ~ (tilde) key and choose the “Front” view. This hotkey is useful for switching to precise and exact camera angles. In our case, it makes you face the front, as if you are looking at a 2D plane, rather than the bird’s eye view we usually get when we open up blender.\\n\\nHold shift and middle mouse key to move your “camera” without accidently getting out of this “Front” 2D-esque view. Use the middle mouse button to zoom in and out until you feel like you have a good view of where your vase would be.\\n\\nShift + A >> Mesh >> Cube. Do this to create a cube (this is assuming you don’t already have one when you boot up blender). Shift + A is the go to hotkey for adding any new models in your blender project. Click the cube after you’ve created it.\\n\\nClick Tab. This takes the object and switches it from “Object Mode” (see the top left of your screen) to “Edit Mode” which allows you to edit the object.\\n\\nClick 1 in edit mode. This changes the mouse section from selecting the cube’s faces or edges (AKA lines on the cube) to only selecting vertices (AKA single points).\\n\\nClick A to select all. This will select all the points at once.\\n\\nPress M and choose “merge at center”. This will merge all the points of the cube into a single point at the center of all the other points.\\n\\nPress the “Move” option on the left side of the viewport on the object tool menu to move the point that is selected. If you can’t see the single point that remains, just press A again and the move tool and you should see it selected. Alternatively, you can press G for the hotkey and press X, Y, or Z to move the point as well.\\n\\nMove the point to a place on the screen where the top right of the vase would be.\\n\\nPress E to extrude lines from that single point and draw the outer edge of the shape of the vase the best you can, pressing E for new lines each time.\\n\\nNow that you have the shape you can hold the middle mouse key and move it to get a better view of this partial “edge loop” of the vase. Edge loops just mean edges that are connected to each other.\\n\\nPress A to select all the vertices that you just drew earlier for the vase. Remember, you must be in Edit Mode to select all these vertices and you can’t get into Edit Mode without first selecting the object in Object Mode.\\n\\nClick the “Spin” tool in the object toolbar menu on the left. It looks like a pie chart with a piece of the pie missing.\\n\\nDrag the plus symbol around a little and then let go of the mouse key.\\n\\nIn the bottom left, there should be a context menu for the object you just spun. Click it and change the number of steps to fit how detailed you want the vase to be (8 steps is good enough because you can always smooth it out later). Additionally, change the Angle to -360 degrees. This will connect the vase around because you spun the line object you just drew into a full circle, creating the vase shape.\\n\\nThe bottom of the vase will be empty. To fill the bottom of the vase with a face (AKA a shape that covers one part of an object like each square on a cube), go to Edit Mode and click 2. This will select edges, rather than vertices, when you click an object. Select the bottom edges by either holding shift and clicking each one OR holding alt and pressing the bottom line to select the edge you clicked along with any edge directly connected with the edge you clicked. The lines will light up when selected.\\n\\nTo thicken the vase, select the vase, go to the bottom right menu and click the Modifier tab (picture looks like a wrench). Add a new modifier and select the Solidify modifier. This thickens the edges of the vase so the hole at the top doesn’t look paper thin. To increase the thickness, drag the thickness bar up. Be sure not to do it too much because that will cause polygons to overlap, creating issues in your vase. To check for these overlapping lines, you can go to the top right (where the 4 spheres are) and select the very left one. This is the wireframe view of the object, which only shows edges and vertices of an object. Alternatively, you can hold Z to change the view to wireframe after bringing up the view menu.\\n\\nLastly, go to Object mode, select the vase and right-click and press Shade Smooth. This will smooth out your object. It will still look a little rough.\\n\\nWith your object selected, go to the modifier tab again on the bottom right, add the Subdivision Surface Modifier, and change it to 2 in the Levels Viewport and Render. The former shows you the smoothed look of the vase in the Viewport while the latter only shows what it looks like during rendering. Its usually good to keep both the same. Please note that modifiers are applied IN ORDER from top to bottom, meaning the Solidify is applied first, and the subdivision surface modifier is applied on TOP of the solidified vase we have.\\n\\nNow the the vase looks too smooth in some parts. Especially places where we need it to be a little less round (such as the bottom, which should be flatter). To sharpen the definition to the vase in these parts that need to be more edgy and less smooth, use Edge Loops. This will make the vase look a little less round on the edges and add definition. To add an Edge Loop, go to Edit Mode, click Ctrl + R to bring up the edge loops, drag the edge loops close, but not exactly to the edges of the vase. This will sharpen the vase in places that make it look more realistic.\\n\\nAfter you are done sharpening the edges of the vase, click Tab to go back to object mode to view your final model. Voila, you’ve completed the basic modeling shape for a vase!\\n\\nCategories Blender\\n\\nTags\\n\\nblender 3d modeling for beginners,\\n\\nblender tutorial,\\n\\nblender tutorials beginner,\\n\\ncreating vase in blender\\n\\nFlutter: How to Hide Keyboard on iPhone & Android\\n\\nExploring Sentiment Analysis with Nepali Text\\n\\n1 thought on “3D Blender Step-By-Step Tutorial: How to Create a Simple Vase in Blender?”\\n\\nEva\\t\\t\\t\\t\\t\\t\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tJuly 31, 2023 at 3:44 am\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\n\\t\\t\\t\\t\\n\\t\\t\\t\\t\\tVery helpful. Thanks!\\n\\nLeave a Comment\\n\\nComment', metadata={'source': 'https://kickerai.com/3d-blender-step-by-step-tutorial-how-to-create-a-simple-vase-in-blender/'}),\n",
              " Document(page_content='Git Worktrees: Why would you use it?\\n\\nAugust 20, 2023\\n\\nby Patrick Methods\\n\\nWhenever you are developing something, context switching is something we all come across. Lucky for us there are multiple ways one can achieve this, even with in Git. Here we will be looking at one of the most useful tool for this, Git Worktree.\\n\\nTable of Contents:\\n\\nWhen to use Git Worktrees?\\n\\nHow does Git Worktree work?\\n\\nHow toCreate WorktreeSwitch WorktreeRemove Worktree\\n\\nConclusion\\n\\nWhen to use Git Worktrees?\\n\\nAfter you spend some time working with git on a project, there comes a situation where you have to switch between multiple branches either for applying some quick fix, work on something of higher priority, or checking a PR/code review, etc. Normally one would use a feature like git-stash or create clones of repos in multiple directories and work othem. But these solutions often have an overhead of time and complexity.\\n\\nNow, what can we do about this? The solution you are looking for may very well be Git Worktrees. This is a feature provided by git that allows you to have multiple branches checked out at the same time by creating multiple clone of a single repository.\\n\\nHow does Git Worktree work?\\n\\nYou can switch to a different branch and do the necessary work without affecting your current work directory. The basic workflow will have a structure like this:\\n\\nCreate a replica of the project and witch to a new branch\\n\\nWork on the task\\n\\nCommit and push\\n\\nBack to the prior working directory and continue your work\\n\\nCreate Worktree\\n\\nLet’s take a look at an example. Suppose you are working on a project. Just for the fun of it, let’s name it Patrick. Now you want to add a feature appendage. To incorporate worktree, you first need to use the command git worktree add .\\xa0This command creates a worktree along with a branch that is named after the final word in your path. Then you can navigate to the path that you provided and start working on the task.\\n\\ngit worktree add <PATH>\\n\\n# Create feature-x directory and branch with the same name.\\ngit worktree add ../appendage\\n\\nWhen you want to give your branch a name then you can use the -b flag\\n\\ngit worktree add -b appendage ../appendage\\n\\u200e\\u200e\\n\\nSwitching Worktree\\n\\nYou can work on many things in parallel, pausing one, starting another, returning back to it and more\\xa0 back and forth.\\n\\nYou can view the list of work trees using list command git worktree list\\n\\nRemoving Worktree\\n\\nAnd to remove any worktree you can use the remove command:\\n\\ngit worktree remove <name-of-worktree>\\n\\ngit worktree remove appendage\\n\\nConclusion\\n\\nGit Worktree is a nifty feature that allows you to switch context easily without altering the main working directory.\\n\\nBut you should be careful as some tools may not fully support it. Worktrees rely on the fact that git allows .git to be a file that points to a directory, instead of an actual directory. Some tools incorrectly assume that .git must be a directory and run into issues with worktrees. Regardless, this is an awesome and convenient tool to have in your tool box.\\n\\nCategories Blog\\n\\nTags\\n\\ngit,\\n\\ngit worktree,\\n\\ngithub,\\n\\nwhat would you use git worktree for,\\n\\nwhy would you use git worktree for\\n\\nHow to Get Started on Flutter | For Both Non Programmers and Programmers\\n\\nLeave a Comment\\n\\nComment', metadata={'source': 'https://kickerai.com/git-worktrees-why-would-you-use-it/'}),\n",
              " Document(page_content='Flutter: How to Hide Keyboard on iPhone & Android\\n\\nAugust 20, 2023\\n\\nby Prasant\\n\\nOne of the most common issues flutters users face while developing mobile apps is the keyboard. After the Keyboard pops up, it is important to dismiss it because it takes up around half the area on the screen.\\n\\nRelated: Login/Register in Flutter Using Firebase\\n\\nIn this tutorial, we will look at how to hide the keyboard on iPhone and Android devices, particularly in a Flutter app, after clicking a button.\\n\\nTable of Contents\\n\\nHow to Hide Keyboard on iPhoneTapping outside the keyboardUsing a toolbar button\\n\\nHow to Hide Keyboard on AndroidUsing a toolbar buttonTapping outside the keyboardOther Flutter Related Articles:\\n\\nHow to Hide Keyboard on iPhone\\n\\nThere are several ways to hide the keyboard on an iPhone. Here are two methods that you can use in your Flutter app:\\n\\nTapping outside the keyboard\\n\\nThe simplest way to hide the keyboard on an iPhone is by tapping outside the keyboard area. This action will dismiss the keyboard, giving more space to the app’s content. You can add this functionality to your Flutter app by adding a GestureDetector widget that listens for taps outside the keyboard area and then dismisses the keyboard.\\n\\nHere’s the code:\\n\\nThis code creates a GestureDetector that listens for taps outside the keyboard area. When the user taps outside the keyboard, the onTap method is called, which calls the unfocus() method on the FocusScope widget. This method hides the keyboard and gives more space to the app’s content.\\n\\nRelated: Create Internet Connection Checker Using Bloc\\n\\nUsing a toolbar button\\n\\nAnother way to hide the keyboard on an iPhone is by using a toolbar button. This button can be placed on the toolbar or the navigation bar of the app and can be used to hide the keyboard with a single tap. To add this functionality to your Flutter app, you can use the TextInputAction.done action with a TextField widget.\\n\\nHere’s the code:\\n\\nThis code creates a TextField widget with the TextInputAction.done action. When the user taps the “Done” button on the keyboard, the keyboard is automatically hidden.\\n\\nRelated: Earn Money by using these Affiliate Program without Website\\n\\nHow to Hide Keyboard on Android\\n\\nUsing a toolbar button\\n\\nLike the iPhone, you can add a toolbar button to hide the keyboard on an Android device. To do this, you can use the TextFormField widget with the textInputAction property set to TextInputAction.done.\\n\\nHere’s the code:\\n\\nThis code creates a TextFormField widget with the textInputAction property set to TextInputAction.done. When the user taps the “Done” button on the keyboard, the keyboard is automatically hidden.\\n\\nRelated: Fetch Data from the Internet Using API in Flutter\\n\\nTapping outside the keyboard\\n\\nLike in IOS, You can use a GestureDetector widget to listen for taps outside the keyboard area, then hide the keyboard using the unfocus() method.\\n\\nHere’s the code:\\n\\nAdditionally, if you want to collapse the keyboard on iOS without tapping on any button, you can use the scrollViewWillBeginDragging method of the UIScrollViewDelegate to detect when the user starts scrolling and dismisses the keyboard. Here’s the code:\\n\\nThis code creates a ScrollController and attaches it to the SingleChildScrollView widget. When the user starts scrolling, the _scrollController listens for the reverse scroll direction and hides the keyboard using the unfocus() method.\\n\\nTo summarize, hiding the keyboard on iPhone and Android is a crucial feature for mobile apps. By using the techniques we’ve explored in this tutorial, we can ensure that our Flutter apps provide a smooth and user-friendly experience for our users.\\n\\nHere are the sources that I used for this tutorial:\\n\\nFlutter documentation: Text Field, Gesture Detector, and Scroll View.\\n\\nMedium article: How to dismiss the keyboard on tap outside in Flutter.\\n\\nStack Overflow question: How to dismiss the keyboard when touching outside of UITextField.\\n\\nOther Flutter Related Articles:\\n\\n[Solved] Don’t use one refreshController to multiple SmartRefresher\\n\\nWhy isn’t Pubsec in dart?\\n\\nUnwrap Key Failed Flutter Error\\n\\nLogin and Register Easily with Flutter using Firebase\\n\\nInternet Connection Checker Using Bloc [ Flutter Code Example ]\\n\\nCategories Flutter\\n\\nTags Flutter, flutter keyboard hide, keyboard dismiss flutter\\n\\nHow to Get Started on Flutter | For Both Non Programmers and Programmers', metadata={'source': 'https://kickerai.com/flutter-how-to-hide-keyboard-on-iphone-android/'}),\n",
              " Document(page_content='Getting Started with Blender: Introduction | Easy Tutorials For Beginners\\n\\nAugust 20, 2023\\n\\nby Prasant\\n\\nOne-Page Notes: Blender Ch 1: What am I looking at?\\n\\nSo you booted up Blender, a free 3D modeling program on par with most paid programs today, thinking you are about to be a hotshot in the 3D modeling world.\\n\\nWell…you might just be. But first, you’ll need some guidance.\\n\\nAs a fellow student of Blender such as yourself, I’ll be learning Blender alongside you and jotting the notes along the way to make it easier for others to catch up, because who better to teach a beginner, than a slightly more experienced beginner?\\n\\nI’m not going to use fancy terminology unless failing to do so would be technically incorrect. I’m just here to help you guys get started. Let’s begin.\\n\\nIntroduction to Blender\\n\\nThis is the blender default viewport menu with some badly drawn highlights:\\n\\nOpen the Image in the new tab for good quality\\n\\nThe green menu is the menu that you use to affect your object (the cube) directly. Its basically what you’ll use to shape your object.\\n\\nThe yellow menu is a menu that contains smaller menus that also affect your object, but its functionality is much more diverse than just shaping the object. This will have its own chapter.\\n\\nThe teal/cyan menu are the workspace menus that are basically preset menus that help you work for specific purposes (such as rendering, sculpting 3d objects, compositing (like photoshop editing but for 3d), and more. Most of the time you will begin with the Layout workspace.\\n\\nThe red menu is the timeline, which is basically the menu you’ll use for animations.\\n\\nThe orange menu is the Properties Editor, which you’ll be using for various specialized functions, such as changing render settings, changing the sky and air of your blender scene, changing particle effects on an object, changing the material of your object, such as subdividing objects (which makes it look more detailed and smooth), and more. The top half of the Properties editor affects the blender scene more generally while the bottom half is more specific to that active object.\\n\\nSo you want to begin navigating through Blender?\\n\\nLearn to follow the 20/80 rule.\\n\\nYou’ll only be using 20% of the entirety of the blender to get 80% of the results. The rest are just additional tools that are helpful, but not necessary for you to know.\\n\\nFind out which tools are the ones you’ll use the most and learn to navigate the blender viewport and menus first before you get started with anything else.\\n\\nYou can also Learn Blender from top teachers in Udemy (Affiliate). Learn Blender in Udemy\\n\\nCategories Blender\\n\\nTags\\n\\nblender 3d modeling for beginners,\\n\\nblender tutorials beginner,\\n\\ngetting started with blender,\\n\\ngetting started with blender tutorial,\\n\\nhow to make a character in blender for beginners\\n\\nHow to Get Started on Flutter | For Both Non Programmers and Programmers\\n\\nLeave a Comment\\n\\nComment', metadata={'source': 'https://kickerai.com/getting-started-with-blender-introduction/'}),\n",
              " Document(page_content='Creating a QRCode scanner with Swift | Swift Tutorial\\n\\nAugust 20, 2023\\n\\nby Sajal Limbu\\n\\nCreating a QRCode scanner in iOS in Swift\\n\\nWe can build a QRCode scanner in iOS very easily using the AVFoundation library provided to us by Apple. It is a very powerful tool to manipulate the Audio and Video capability of the device.\\n\\nBut today we’ll only be using the MetaDataObject api to detect QRCode and read the data from it.\\n\\nWhat we’ll build\\n\\nTable of Contents:\\n\\nCreating a New Project\\n\\nBuilding the UI\\n\\nAdding the Functionality\\n\\nDetecting the QRcodes\\n\\nFinal Codes\\n\\nAlso Check out: Building a Todo App with SwiftUI\\n\\nCreating a New Project\\n\\nWe can start by creating a new iOS project.\\n\\nWe now need to add a new key to our Info.plist. Privacy – Camera Usage Description and give it a value QRScan needs camera access to read QRCode. This message will show up when the app is asking for Camera permissions from the user. The permission is only asked once and can be reset in settings.\\n\\nIf you want to make it dynamic, we can use a placeholder to replace the app name.\\n\\n$(PRODUCT_NAME) needs camera access to read QRCode.\\n\\nBuilding the UI of the App\\n\\nWe can now start building the UI of our app. Go to the Main.storyboard and add an ImageView. We’ll later use this image view to capture the data.\\n\\nAdd constraints to the image view, set width to 250 and height to 250.\\n\\nWe’ll also add some alignment constraints and set true for both Horizontal and Vertical in container. This will make sure that the image view is always at the center of the screen.\\n\\nUsing Assistant, drag the new image view to the respective view controller file and give it a name. I’ll name it scanimageview. Make sure that the view is annotated as IBOutlet.\\n\\nAdding the Functionality\\n\\nTo tap into the AV Foundation apis, we need to first import the library.\\n\\nAdd import AVFoundation just below import UIKit\\n\\nWe’ll also have to set the delegate for MetaDataOutput api. Add AVCaptureMetadataOutputObjectsDelegate as shown.\\n\\nTo start capturing data, we need to first create a session.\\n\\nHere, we have created an AVCaptureSession and assigned it to session constant. We’ll also need to initialize a preview layer that’ll be used to capture video data.\\n\\nWhen the view is first loaded, we need to start capturing the data coming from the camera. We can do this by creating an AVCaptureDevice instance.\\n\\nlet captureDevice = AVCaptureDevice.default(for: AVMediaType.video)        \\n do {\\n      let input = try AVCaptureDeviceInput(device: captureDevice!)\\n      session.addInput(input)\\n  } catch {\\n      print(\"Error capturing QRCode\")\\n  }\\n\\nThe capture instance is then added as a session input. We also create an output instance and add it to the session as well.\\n\\nlet output = AVCaptureMetadataOutput()\\n session.addOutput(output)\\n\\nWe can then set the MetaDataObject delegate methods that’ll be triggered when the QRcode is detected through the input.\\n\\noutput.setMetadataObjectsDelegate(self, queue: DispatchQueue.main)\\noutput.metadataObjectTypes = [AVMetadataObject.ObjectType.qr]\\n\\nThe previewLayer is added for the session with it’s frame stretching all the way to the bounds of the root view. It is added as a subview to the root view layer so that when our image view is overlapped on top, it creates a border.\\n\\npreviewLayer = AVCaptureVideoPreviewLayer(session: session)\\n  previewLayer.frame = view.layer.bounds\\n  view.layer.addSublayer(previewLayer)\\n        \\n  scanimageview.layer.borderWidth = 2\\n  scanimageview.layer.borderColor = UIColor.red.cgColor\\n  self.view.bringSubviewToFront(scanimageview)\\n\\nDetecting the QRcodes\\n\\nAfter all the configuration, we can now start our capturing session and detect QR codes.\\n\\nsession.startRunning()\\n\\nBut first, we’ll need to add our delegate method to handle the returned object data.\\n\\nfunc metadataOutput(_ output: AVCaptureMetadataOutput, didOutput metadataObjects: [AVMetadataObject], from connection: AVCaptureConnection) {\\n        if let metaDataObject = metadataObjects.first {\\n            guard let readableObject = metaDataObject as? AVMetadataMachineReadableCodeObject else {\\n                return\\n            }\\n            let alert = UIAlertController(title: \"QRCode\", message: readableObject.stringValue, preferredStyle: .actionSheet)\\n            \\n            alert.addAction(UIAlertAction(title: \"ok\", style: .default, handler: { action in\\n                self.session.startRunning()\\n            }))\\n            \\n            present(alert, animated: true, completion: nil)\\n        }\\n    }\\n\\nTyping metadataOutput and using the first suggestion for auto-complete, should yield you the above function.\\n\\nHere, we’ll read the metaDataObjects, which is an array of data of the QR code scanned. The data we need is always stored in the first index of the array.\\n\\nWe’ll use an “if let” to unwrap the value. We can then upcast(as?) the readableObject to AVMetadataMachineReadableCodeObject.\\n\\nTo show the obtained value, we’ll use an alertsheet. For this, we need to create an alert using a UIAlertController. We can then add some actions on the alert. Lastly, we present the alert object which will show an alert to the user.\\n\\nFinal Code:\\n\\n//\\n//  ViewController.swift\\n//  QRscan\\n//\\n//  Created by Sajal Limbu on 03/07/2021.\\n//\\n\\nimport UIKit\\nimport AVFoundation\\n\\nclass ViewController: UIViewController, AVCaptureMetadataOutputObjectsDelegate {\\n\\n    @IBOutlet weak var scanimageview: UIImageView!\\n    \\n    let session = AVCaptureSession()\\n    var previewLayer = AVCaptureVideoPreviewLayer()\\n    \\n    override func viewDidLoad() {\\n        super.viewDidLoad()\\n    \\n        let captureDevice = AVCaptureDevice.default(for: AVMediaType.video)\\n        \\n        do {\\n            let input = try AVCaptureDeviceInput(device: captureDevice!)\\n            session.addInput(input)\\n        } catch {\\n            print(\"Error capturing QRCode\")\\n        }\\n        \\n        let output = AVCaptureMetadataOutput()\\n        session.addOutput(output)\\n        \\n        output.setMetadataObjectsDelegate(self, queue: DispatchQueue.main)\\n        output.metadataObjectTypes = [AVMetadataObject.ObjectType.qr]\\n\\nThanks for reading. Check out my other article: Building a Todo App with SwiftUI\\n\\nCategories Blog\\n\\nTags\\n\\ncreate QRcode scanner in IOS,\\n\\nhow to create QRcode scanner in IOS,\\n\\nQRcode scanner using swift tutorial,\\n\\nswift tutorials to create QRcode scanner\\n\\nHow to Get Started on Flutter | For Both Non Programmers and Programmers\\n\\nLeave a Comment\\n\\nComment', metadata={'source': 'https://kickerai.com/creating-a-qrcode-scanner-in-ios-in-swift-swift-tutorial/'}),\n",
              " Document(page_content='Login and Register Easily with Flutter using Firebase\\n\\nAugust 20, 2023\\n\\nby Prasant\\n\\nUser Authentication can be seen in every application nowadays. So Authentication is important for everyone, who is learning Flutter, to know.\\n\\nIn this article, I will be discussing how you can log in and register in Flutter.\\n\\nWe will be using the Firebase console for this. I will be discussing how you can authenticate in flutter using Email only.\\n\\nDon’t worry I will be discussing log in using Google and Facebook in future articles.\\n\\nTable of Contents:\\n\\nCreating a Project in Firebase Console\\n\\nDesigning a Simple UI for the app\\n\\nAdding the packages to Pubsec.yaml\\n\\nHow to register in Flutter using email and password\\n\\nHow to login in Flutter using email and password\\n\\nHow to sign out of the application\\n\\nNow Let’s get started.\\n\\nFlutter Login and Registration using Firebase\\n\\nFirst Step: Creating a Project in Firebase Console\\n\\nWe will be creating/adding a New Project in Firebase Console. If you don’t have an account, you can create one easily.\\n\\nWrite the Project Name and click Continue\\n\\nWe won’t be needing Google Analytics for this test. Disable Google Analytics, and click on Create Project\\n\\nThe Project is now created. Click on Continue\\n\\nGo to the Project Overview and Click on the android or iOS icon to continue.\\n\\nAfter you click on the Android/IOS icon, you will see this page. You have to put the name of your Android/IOS Package which is your applicationId.\\n\\nYou can find the name on android/app/build.gradle\\n\\ndefaultConfig {\\n        // TODO: Specify your own unique Application ID (https://developer.android.com/studio/build/application-id.html).\\n        applicationId \"com.example.login_logout_app\" //this is the name\\n        minSdkVersion 21\\n        targetSdkVersion 30\\n        versionCode flutterVersionCode.toInteger()\\n        versionName flutterVersionName\\n        multiDexEnabled true\\n    }\\n\\nNow you can leave the SHA-1 field empty because in our case we will only be logging with email and password. But if you have to log in with google or Facebook, you need to put the SHA-1.\\n\\nHere’s how you can find the SHA-1 certificate. Here’s the link to the StackFlow:  Where do I get SHA-1 certificate fingerprint using flutter?\\n\\nNow Click on the Register app.\\n\\nYou have to download the google-services.json file and put that file in the android/app folder\\n\\nThe next step will be putting all those lines of code exactly as it is in the specified location.\\n\\nNow you are all done. Click on Continue to console.\\n\\nClick on Authentication in the dashboard. You will then see this page.\\n\\nEnable the Email/Password and Click on Save.\\n\\nSecond Step: Designing a Simple UI for the app\\n\\nWe will be creating a simple UI for the user to Log in, Sign up, and HomeScreen.\\n\\nLogin_Screen where user can log in with Email and Password\\n\\nRegister_Screen to register the user using email and password\\n\\nHome_Screen which the user sees after successfully logging in with email and password\\n\\nThird Step: Adding the packages to Pubsec.yaml\\n\\nAdding the following Packages to pubsec.yaml\\n\\nfirebase_auth: ^3.0.1\\n  firebase_core: ^1.4.0\\n\\nFourth Step: Register using email and password\\n\\nRegister_Screen.dart\\n\\nHere are important fragments of codes from Register_Screen. You can find the full code on Github.\\n\\n//1\\nimport \\'package:firebase_auth/firebase_auth.dart\\';\\n      //2\\n  final _auth = FirebaseAuth.instance;\\n  String email = \\'\\';\\n  String password = \\'\\';\\n      //3\\n                            TextFormField(\\n                              keyboardType: TextInputType.emailAddress,\\n                              onChanged: (value) {\\n                                email = value.toString().trim();\\n                              },\\n                              textAlign: TextAlign.center,\\n                              decoration: kTextFieldDecoration.copyWith(\\n                                hintText: \\'Enter Your Email\\',\\n                                prefixIcon: Icon(\\n                                  Icons.email,\\n                                  color: Colors.black,\\n                                ),\\n                              ),\\n                            ),\\n              //4\\n                            TextFormField(\\n                              obscureText: true,\\n                              validator: (value) {\\n                                if (value!.isEmpty) {\\n                                  return \"Please enter Password\";\\n                                }\\n                              },\\n                              onChanged: (value) {\\n                                password = value;\\n                              },\\n                              textAlign: TextAlign.center,\\n                              decoration: kTextFieldDecoration.copyWith(\\n                                  hintText: \\'Choose a Password\\',\\n                                  prefixIcon: Icon(\\n                                    Icons.lock,\\n                                    color: Colors.black,\\n                                  )),\\n                            ),\\n           //5\\n                            LoginSignupButton(\\n                              title: \\'Register\\',\\n                              ontapp: () async {\\n            (i)           \\n                                  try {\\n                                    await _auth.createUserWithEmailAndPassword(\\n                                        email: email, password: password);\\n                                    ScaffoldMessenger.of(context).showSnackBar(\\n                                      SnackBar(\\n                                          child: Text(\\n                                              \\'Sucessfully Register.You Can Login Now\\'),\\n                                        ),\\n                                        duration: Duration(seconds: 5),\\n                                      ),\\n                                    );\\n                                    Navigator.of(context).pop();\\n                  (ii)\\n                                  } on FirebaseAuthException catch (e) {\\n                                    showDialog(\\n                                      context: context,\\n                                      builder: (ctx) => AlertDialog(\\n                                        title:\\n                                            Text(\\' Ops! Registration Failed\\'),\\n                                        content: Text(\\'${e.message}\\'),\\n                                      ......\\n\\nFirst (1), we will be importing the Firebase Authentication package to our Register_Screen.\\n\\nAfter that (2), we will be creating an instance of FirebaseAuth and assign its value to _auth which is used as Global value. We will be also declaring email and password as String and they will have empty values.\\n\\n(3) is for the User to enter the email.\\n\\n(4) is for the User to enter the password.\\n\\nThe entered value(email) is stored in the email String and the value(password) is stored in the password String.\\n\\nAt (5) where we have made Register Button. (i) The main work is to register the user after he has entered the email and password.\\n\\nRegister button calls for the method: createUserWithEmailAndPassword(). This method stores our email and password in the firebase console.\\n\\nAfter registering, we will be directed to the Login_Screen. The snack bar is shown to the user at the bottom with the message of successful registration.\\n\\nThe credentials are stored in Firebase with the createUserWithEmailAndPassword() method which is accessed from _auth.\\n\\n(ii) If any error occurs, the error/exception is caught in the catch block, and the Dialog box is shown informing the user of the error.\\n\\nFifth Step: Login using Email and Password\\n\\nHere are important fragments of codes from Login_Screen. You can find the full code on Github.\\n\\nThe codes are similar to Register_Screen codes.\\n\\nThese are the most important code that you should pay attention to.\\n\\nLogin_Screen.dart\\n\\n//1\\nimport \\'package:firebase_auth/firebase_auth.dart\\';\\n.....\\n//2\\n final _auth = FirebaseAuth.instance;\\n  String email = \\'\\';\\n  String password = \\'\\';\\n.....\\n.....\\n//3\\n                            TextFormField(\\n                              keyboardType: TextInputType.emailAddress,\\n                              onChanged: (value) {\\n                                email = value;\\n                              },\\n                              },\\n                              textAlign: TextAlign.center,\\n                              decoration: kTextFieldDecoration.copyWith(\\n                                hintText: \\'Email\\',\\n                                prefixIcon: Icon(\\n                                  Icons.email,\\n                                  color: Colors.black,\\n                                ),\\n                              ),\\n                            ),\\n                            SizedBox(height: 30),\\n//4\\n                            TextFormField(\\n                              obscureText: true,\\n                              onChanged: (value) {\\n                                password = value;\\n                              },\\n                              textAlign: TextAlign.center,\\n                              decoration: kTextFieldDecoration.copyWith(\\n                                  hintText: \\'Password\\',\\n                                  prefixIcon: Icon(\\n                                    Icons.lock,\\n                                    color: Colors.black,\\n                                  )),\\n                            ),\\n                            SizedBox(height: 80),\\n//5\\n                            LoginSignupButton(\\n                              title: \\'Login\\',\\n                              ontap: () async {\\n                                             try {\\n                                    await _auth.signInWithEmailAndPassword(\\n                                        email: email, password: password);\\n                                    await Navigator.of(context).push(\\n                                      MaterialPageRoute(\\n                                        builder: (contex) => HomeScreen(),\\n                                      ),\\n                                    );\\n                                                 } on FirebaseAuthException catch (e) {\\n                                    showDialog(\\n                                      context: context,\\n                                      builder: (ctx) => AlertDialog(\\n                                        title: Text(\"Ops! Login Failed\"),\\n                                        content: Text(\\'${e.message}\\'),\\n......\\n\\nFirst (1), we will be importing the Firebase Authentication package to our Login_Screen.\\n\\nAfter that (2), we will be creating an instance of FirebaseAuth and assign its value to _auth which is used as Global value. We will be also declaring email and password as String and they will have empty values.\\n\\n(3) is for the User to enter the email.\\n\\n(4) is for the User to enter the password.\\n\\nThe entered value(email) is stored in the email String and the value(password) is stored in the password String.\\n\\nAt (5) where we have made Login Button.\\n\\nLoginSignupButton(\\n                              title: \\'Login\\',\\n                              ontap: () async {\\n                                        //i\\n                                             try {\\n                                    await _auth.signInWithEmailAndPassword(\\n                                        email: email, password: password);\\n                                    await Navigator.of(context).push(\\n                                      MaterialPageRoute(\\n                                        builder: (contex) => HomeScreen(),\\n                                      ),\\n                                    );\\n                                         //ii\\n                                                 } on FirebaseAuthException catch (e) {\\n                                    showDialog(\\n                                      context: context,\\n                                      builder: (ctx) => AlertDialog(\\n                                        title: Text(\"Ops! Login Failed\"),\\n                                        content: Text(\\'${e.message}\\'),\\n\\nAfter the user has entered email and password, he will tap on the Login button.\\n\\nLogin button calls for the method: _signInWithEmailAndPassword(). This method (i) is where we will pass the email and password to Firebase for authentication and retrieve the FirebaseUser object. This will match the email and password with the data stored in the firebase.\\n\\nIf the records are correct we will be directed to the Home_Screen.\\n\\nIf the records are incorrect, an exception will occur which will show a dialog box.\\n\\nThe authentication and showing exceptions are done inside the try and catch block as you can see in the code above.\\n\\nSixth Step: Signing out of the app\\n\\nHome_Screen\\n\\nThe Home_Screen has a very simple UI and there is only one button for logout.\\n\\nThe implementation is simple as well. We first have to create the instance of Firebase and assign it to _firebaseAuth.\\n\\nWe will then create a simple asynchronous method _signOut() and access the signOut object from the instance _firebaseAuth.\\n\\nNext, the signOut() method will be called when the user presses on the Log out button. The user is then logged out.\\n\\nHome_Screen.dart\\n\\nimport \\'package:firebase_auth/firebase_auth.dart\\';\\nimport \\'package:flutter/material.dart\\';\\nimport \\'package:login_logout_app/Screens/Login_Screen.dart\\';\\nclass HomeScreen extends StatefulWidget {\\n  @override\\n  _HomeScreenState createState() => _HomeScreenState();\\n}\\nfinal FirebaseAuth _firebaseAuth = FirebaseAuth.instance;\\n_signOut() async {\\n  await _firebaseAuth.signOut();\\n}\\nclass _HomeScreenState extends State<HomeScreen> {\\n  @override\\n  Widget build(BuildContext context) {\\n    return Scaffold(\\n      backgroundColor: Colors.grey[200],\\n      body: Center(\\n        child: Column(\\n          mainAxisAlignment: MainAxisAlignment.center,\\n          children: [\\n            Text(\\'You have logged in Successfuly\\'),\\n            SizedBox(height: 50),\\n            Container(\\n              height: 60,\\n              width: 150,\\n              child: ElevatedButton(\\n                  clipBehavior: Clip.hardEdge,\\n                  child: Center(\\n                    child: Text(\\'Log out\\'),\\n                  ),\\n                  onPressed: () async {\\n                    await _signOut();\\n                    if (_firebaseAuth.currentUser == null) {\\n                      Navigator.push(\\n                        context,\\n                        MaterialPageRoute(builder: (context) => LoginScreen()),\\n                      );\\n                    }\\n                  }),\\n            )\\n          ],\\n        ),\\n      ),\\n    );\\n  }\\n}\\n\\nNow we are all done with authentication. Hope I was able to help you out and you were able to learn about register, log in, and log out in flutter.\\n\\nHere’s the Github link to this project: Login_Logout_App\\n\\nI will be writing about how you can log in using Google and Facebook in coming articles.\\n\\nThanks for reading.\\n\\nRegister_Screen Code\\n\\nimport \\'package:firebase_auth/firebase_auth.dart\\';\\nimport \\'package:flutter/material.dart\\';\\nimport \\'package:flutter/services.dart\\';\\nimport \\'package:login_logout_app/Component/button.dart\\';\\nimport \\'../constants.dart\\';\\nimport \\'Login_Screen.dart\\';\\nclass SignupScreen extends StatefulWidget {\\n  @override\\n  _SignupScreenState createState() => _SignupScreenState();\\n}\\nclass _SignupScreenState extends State<SignupScreen> {\\n  final formkey = GlobalKey<FormState>();\\n  final _auth = FirebaseAuth.instance;\\n  String email = \\'\\';\\n  String password = \\'\\';\\n  bool isloading = false;\\n  @override\\n  Widget build(BuildContext context) {\\n    return Scaffold(\\n      appBar: AppBar(\\n        backgroundColor: Colors.grey[200],\\n        elevation: 0,\\n        leading: IconButton(\\n          icon: Icon(Icons.arrow_back_ios, color: Colors.black,size: 30,),\\n          onPressed: () => Navigator.of(context).pop(),\\n        ),\\n      ),\\n      body: isloading\\n          ? Center(\\n              child: CircularProgressIndicator(),\\n            )\\n          : Form(\\n              key: formkey,\\n              child: AnnotatedRegion<SystemUiOverlayStyle>(\\n                value: SystemUiOverlayStyle.light,\\n                child: Stack(\\n                  children: [\\n                    Container(\\n                      height: double.infinity,\\n                      width: double.infinity,\\n                     color: Colors.grey[200],\\n                      child: SingleChildScrollView(\\n                        padding:\\n                            EdgeInsets.symmetric(horizontal: 25, vertical: 120),\\n                        child: Column(\\n                          mainAxisAlignment: MainAxisAlignment.center,\\n                          children: [\\n                            Hero(\\n                              tag: \\'1\\',\\n                                                          child: Text(\\n                                \"Sign up\",\\n                                style: TextStyle(\\n                                    fontSize: 30,\\n                                    color: Colors.black,\\n                                    fontWeight: FontWeight.bold),\\n                              ),\\n                            ),\\n                            SizedBox(height: 30),\\n                            TextFormField(\\n                              keyboardType: TextInputType.emailAddress,\\n                              onChanged: (value) {\\n                                email = value.toString().trim();\\n                              },\\n                              validator: (value) => (value!.isEmpty)\\n                                  ? \\' Please enter email\\'\\n                                  : null,\\n                              textAlign: TextAlign.center,\\n                              decoration: kTextFieldDecoration.copyWith(\\n                                hintText: \\'Enter Your Email\\',\\n                                prefixIcon: Icon(\\n                                  Icons.email,\\n                                  color: Colors.black,\\n                                ),\\n                              ),\\n                            ),\\n                            SizedBox(height: 30),\\n                            TextFormField(\\n                              obscureText: true,\\n                              validator: (value) {\\n                                if (value!.isEmpty) {\\n                                  return \"Please enter Password\";\\n                                }\\n                              },\\n                              onChanged: (value) {\\n                                password = value;\\n                              },\\n                              textAlign: TextAlign.center,\\n                              decoration: kTextFieldDecoration.copyWith(\\n                                  hintText: \\'Choose a Password\\',\\n                                  prefixIcon: Icon(\\n                                    Icons.lock,\\n                                    color: Colors.black,\\n                                  )),\\n                            ),\\n                            SizedBox(height: 80),\\n                            LoginSignupButton(\\n                              title: \\'Register\\',\\n                              ontapp: () async {\\n                                if (formkey.currentState!.validate()) {\\n                                  setState(() {\\n                                    isloading = true;\\n                                  });\\n                                  try {\\n                                    await _auth.createUserWithEmailAndPassword(\\n                                        email: email, password: password);\\n                                    ScaffoldMessenger.of(context).showSnackBar(\\n                                      SnackBar(\\n                                        backgroundColor: Colors.blueGrey,\\n                                        content: Padding(\\n                                          padding: const EdgeInsets.all(8.0),\\n                                          child: Text(\\n                                              \\'Sucessfully Register.You Can Login Now\\'),\\n                                        ),\\n                                        duration: Duration(seconds: 5),\\n                                      ),\\n                                    );\\n                                    Navigator.of(context).pop();\\n                                    setState(() {\\n                                      isloading = false;\\n                                    });\\n                                  } on FirebaseAuthException catch (e) {\\n                                    showDialog(\\n                                      context: context,\\n                                      builder: (ctx) => AlertDialog(\\n                                        title:\\n                                            Text(\\' Ops! Registration Failed\\'),\\n                                        content: Text(\\'${e.message}\\'),\\n                                        actions: [\\n                                          TextButton(\\n                                            onPressed: () {\\n                                              Navigator.of(ctx).pop();\\n                                            },\\n                                            child: Text(\\'Okay\\'),\\n                                          )\\n                                        ],\\n                                      ),\\n                                    );\\n                                  }\\n                                  setState(() {\\n                                    isloading = false;\\n                                  });\\n                                }\\n                              },\\n                            ),\\n                          ],\\n                        ),\\n                      ),\\n                    )\\n                  ],\\n                ),\\n              ),\\n            ),\\n    );\\n  }\\n}\\n\\nLogin_Screen Codes\\n\\nimport \\'package:flutter/material.dart\\';\\nimport \\'package:flutter/services.dart\\';\\nimport \\'package:firebase_auth/firebase_auth.dart\\';\\nimport \\'package:login_logout_app/Component/button.dart\\';\\nimport \\'../constants.dart\\';\\nimport \\'Home_Screen.dart\\';\\nimport \\'Register_Screen.dart\\';\\nclass LoginScreen extends StatefulWidget {\\n  @override\\n  _LoginScreenState createState() => _LoginScreenState();\\n}\\nclass _LoginScreenState extends State<LoginScreen> {\\n  final formkey = GlobalKey<FormState>();\\n  final _auth = FirebaseAuth.instance;\\n  String email = \\'\\';\\n  String password = \\'\\';\\n  bool isloading = false;\\n  @override\\n  Widget build(BuildContext context) {\\n    return Scaffold(\\n      body: isloading\\n          ? Center(\\n              child: CircularProgressIndicator(),\\n            )\\n          : Form(\\n              key: formkey,\\n              child: AnnotatedRegion<SystemUiOverlayStyle>(\\n                value: SystemUiOverlayStyle.light,\\n                child: Stack(\\n                  children: [\\n                    Container(\\n                      height: double.infinity,\\n                      width: double.infinity,\\n                      color: Colors.grey[200],\\n                      child: SingleChildScrollView(\\n                        padding:\\n                            EdgeInsets.symmetric(horizontal: 25, vertical: 120),\\n                        child: Column(\\n                          mainAxisAlignment: MainAxisAlignment.center,\\n                          children: [\\n                            Text(\\n                              \"Sign In\",\\n                              style: TextStyle(\\n                                  fontSize: 50,\\n                                  color: Colors.black,\\n                                  fontWeight: FontWeight.bold),\\n                            ),\\n                            SizedBox(height: 30),\\n                            TextFormField(\\n                              keyboardType: TextInputType.emailAddress,\\n                              onChanged: (value) {\\n                                email = value;\\n                              },\\n                              validator: (value) {\\n                                if (value!.isEmpty) {\\n                                  return \"Please enter Email\";\\n                                }\\n                              },\\n                              textAlign: TextAlign.center,\\n                              decoration: kTextFieldDecoration.copyWith(\\n                                hintText: \\'Email\\',\\n                                prefixIcon: Icon(\\n                                  Icons.email,\\n                                  color: Colors.black,\\n                                ),\\n                              ),\\n                            ),\\n                            SizedBox(height: 30),\\n                            TextFormField(\\n                              obscureText: true,\\n                              validator: (value) {\\n                                if (value!.isEmpty) {\\n                                  return \"Please enter Password\";\\n                                }\\n                              },\\n                              onChanged: (value) {\\n                                password = value;\\n                              },\\n                              textAlign: TextAlign.center,\\n                              decoration: kTextFieldDecoration.copyWith(\\n                                  hintText: \\'Password\\',\\n                                  prefixIcon: Icon(\\n                                    Icons.lock,\\n                                    color: Colors.black,\\n                                  )),\\n                            ),\\n                            SizedBox(height: 80),\\n                            LoginSignupButton(\\n                              title: \\'Login\\',\\n                              ontapp: () async {\\n                                if (formkey.currentState!.validate()) {\\n                                  setState(() {\\n                                    isloading = true;\\n                                  });\\n                                  try {\\n                                    await _auth.signInWithEmailAndPassword(\\n                                        email: email, password: password);\\n                                    await Navigator.of(context).push(\\n                                      MaterialPageRoute(\\n                                        builder: (contex) => HomeScreen(),\\n                                      ),\\n                                    );\\n                                    setState(() {\\n                                      isloading = false;\\n                                    });\\n                                  } on FirebaseAuthException catch (e) {\\n                                    showDialog(\\n                                      context: context,\\n                                      builder: (ctx) => AlertDialog(\\n                                        title: Text(\"Ops! Login Failed\"),\\n                                        content: Text(\\'${e.message}\\'),\\n                                        actions: [\\n                                          TextButton(\\n                                            onPressed: () {\\n                                              Navigator.of(ctx).pop();\\n                                            },\\n                                            child: Text(\\'Okay\\'),\\n                                          )\\n                                        ],\\n                                      ),\\n                                    );\\n                                    print(e);\\n                                  }\\n                                  setState(() {\\n                                    isloading = false;\\n                                  });\\n                                }\\n                              },\\n                            ),\\n                            SizedBox(height: 10),\\n                            GestureDetector(\\n                              onTap: () {\\n                                Navigator.of(context).push(\\n                                  MaterialPageRoute(\\n                                    builder: (context) => SignupScreen(),\\n                                  ),\\n                                );\\n                              },\\n                              child: Row(\\n                                children: [\\n                                  Text(\\n                                    \"Don\\'t have an Account ?\",\\n                                    style: TextStyle(\\n                                        fontSize: 20, color: Colors.black87),\\n                                  ),\\n                                  SizedBox(width: 10),\\n                                  Hero(\\n                                    tag: \\'1\\',\\n                                    child: Text(\\n                                      \\'Sign up\\',\\n                                      style: TextStyle(\\n                                          fontSize: 21,\\n                                          fontWeight: FontWeight.bold,\\n                                          color: Colors.black),\\n                                    ),\\n                                  )\\n                                ],\\n                              ),\\n                            )\\n                          ],\\n                        ),\\n                      ),\\n                    )\\n                  ],\\n                ),\\n              ),\\n            ),\\n    );\\n  }\\n}\\n\\nCategories Flutter, featured\\n\\nTags\\n\\nflutter authentication using firebase,\\n\\nflutter for beginners,\\n\\nflutter login and register,\\n\\nflutter login/logout example,\\n\\nhow to get started with flutter\\n\\nHow to Get Started on Flutter | For Both Non Programmers and Programmers\\n\\nLeave a Comment\\n\\nComment', metadata={'source': 'https://kickerai.com/login-and-register-easily-with-flutter-using-firebase/'}),\n",
              " Document(page_content='Easy Blender Tutorials: Things to Avoid when Starting out as a Beginner\\n\\nAugust 20, 2023\\n\\nby Prasant\\n\\nBlender Chapter 2: Things to Avoid When Starting Out\\n\\nIf you haven’t checked out Blender Chapter 1: Introduction. You should first check that out before reading this. Chapter 1, talks about the basics of the Blender and its functionalities. Chapter 1: Introduction to Blender\\n\\nKnowing how the basic interface and hotkeys, which you’ll familiarize yourself with practice, is good to get you started, but during your modeling journey, you are sure to run into situations where you are not sure why your models look…off.\\n\\nThe solution to these problems may lie with the knowledge and good habits that you should integrate into your everyday modeling practice when starting out.\\n\\nHere are some common good habits to build in Blender:\\n\\nApplying your transform:\\n\\nWhenever you manipulate the location, scale, or rotation, an issue comes up where the objects visuals/modeling of the object you are affecting do not quite behave as you intend.\\n\\nThis is because any modifiers or tools you add to that object are directly affected by your transform (location, scale, rotation).\\n\\nFor example, let’s say you have a square plane, and you scale the plane horizontally on the x-axis, thus creating a rectangle. Now let’s say you want to add a bevel to curve the edges of that rectangle so its shape would look something akin to an I-phone. However, when you add the bevel you notice that it doesn’t curve the edges to round out the 4 corners to create that I-phone look. Instead, it ends up getting more and more “oval” like.\\n\\nThis is because the bevel is being directly affected by the scale that was applied to the original square. Instead of beveling a rectangle, you are actually beveling a stretched square scaled to look like a rectangle.\\n\\nTo fix this you apply the transform, which essentially resets all the scaling, location, and rotation to zero values, telling the object that is being modified to become what you see it as, which is an actual rectangle, and not a stretched square. This time, when you apply a bevel to smooth out the corners, it will actually smooth it out evenly, and voila, you have your I-phone shape.\\n\\nFrom left to right: Square, Rectangle/Stretched Square, Stretched Square w/ Bevel Modifier attached, Rectangle (AKA stretched square after Applying Transform) w/ Bevel Modifier\\n\\nAvoid Improper N-gons:\\n\\nAnother good rule of thumb is to avoid using shapes other than quadrilaterals (4-sided planes like rectangles) and triangles (when appropriate), unless confident in your use of more advanced geometry.\\n\\nHowever, if you tamper with geometry, there has to be a consistency about it so that when you edit it, the geometry doesn’t go haywire in ways you didn’t expect.\\n\\nLet’s take a cylinder for example. The tops and bottoms of a cylinder are circles and the two circles are bridged together to create that soda-can-esque shape. Now let’s say you want to use this cylinder to make a pencil. Well, to do that, you would need extra geometry added at the top of the cylinder in order to mold it into a cone-like shape with a point at the top to act as the pencil head.\\n\\nThis top circle would thus have to consist of various polygons (n-gons), which could be anything from a triangle to an octagon or more. But no matter which n-gon you use, know that a circle doesn’t magically turn into a cone because you will it to.\\n\\nIt needs the right type and number of n-gons inside that circle, which you can essentially pull up from the center and create the cone shape that you seek. The question is: Which shape is the best for this? Well, let’s use squares and rectangles only for example (quads). This is the wrong shape, but it’s good to explain why it’s the wrong shape here. If the circle on top is made up of only rectangles and squares, this means as you continue including quads near the edges of the circle, you’ll certainly have to have triangles at some point because there is physically no space near the edges of the circle to allow for a 4-sided shape. This inconsistent geometry usage will cause a lot of weird things to occur.\\n\\nLet’s say you had an invisible hand pull from the center of the circle to create the pencil point. As you drag the geometry up, consisting of the various quads and triangles I mentioned above, the modeling mesh will attempt its best to create that cone-like pencil head, but the cone will end up inconsistently lopsided because the shapes and use of geometry we used were inconsistent and asymmetrical. There is no “center” point in the top circle to create the pencil head from because the squares and rectangles do not create a symmetrical and consistent topology. Dragging it upwards will only result in an “almost”-cone shape that has multiple bumps, grooves, and other asymmetry resulting from the various shapes trying to mold itself into a cone.\\n\\nInstead, the top circle should only be consisted of triangles focusing in on a center point, with some supporting quads encircling the triangles at the center, so when the invisible hand drags the shape upwards, it would actually pull directly from the center point and drag the triangles and quads to form the pencil shape you see below. You can tell just by looking at it that this symmetrical use of quads and triangles with the focus at the center is what keeps this pencil head consistent. Using a chessboard pattern of quads wouldn’t be able to achieve the same results. You’re more likely to get a mountain shape than a pencil head shape using that kind of method.\\n\\nIn short, use the right type and number of geometry and use it consistently throughout your models. Good topology usually focuses on triangles and quads, and never exceeds that unless you are confident in your use of other n-gons such as hexagons, octagons, etc. Common sense and getting used to knowing which shapes work where is the key to avoiding mistakes later down the line.\\n\\nCategories Blender\\n\\nTags\\n\\nblender 3d modeling for beginners,\\n\\nblender for beginners,\\n\\nblender mistakes,\\n\\nblender tutorials beginner,\\n\\nthings to avoid in blender\\n\\nHow to Get Started on Flutter | For Both Non Programmers and Programmers\\n\\nLeave a Comment\\n\\nComment', metadata={'source': 'https://kickerai.com/getting-started-with-blender-things-to-avoid-when-starting-out/'}),\n",
              " Document(page_content='Easy Blender 3D Tutorial: Understanding Spline Modeling in Blender\\n\\nAugust 20, 2023\\n\\nby Prasant\\n\\nChapter 4: Spline Modeling\\n\\nWhat is Spline Modelling?\\n\\nSpline modeling is, in short, modeling against curves. You may have heard of spline modeling involving Nurbs, Beziers, etc. Since this tutorial series is nontechnical in nature, I’ll simplify the basics of spline modeling as much as possible.\\n\\nTo summarize, these splines/curves can be used to generate specific types of objects that utilize curves in their structure. Think of objects like a Raging Waters slide, a magical scroll, an archway, etc. Splines can also be used to determine which direction a particular object will move by constraining that object to the curve. For this, imagine the object is a car and you want to constrain it to the path of a race track during an animation sequence. A spline can be used to force the car to move on a certain curved path without having to animate each frame of the car.\\n\\nCheck out Other Chapters:\\n\\nChapter 1: Getting Started with Blender: Introduction\\n\\nChapter 2: Things to Avoid When Starting Out\\n\\nChapter 3: 3D Blender Step-By-Step Tutorial: How to Create a Simple Vase in Blender?\\n\\nHow do you make a Spline in the blender?\\n\\nMaking a simple curve using Paths\\n\\nLet’s use “Path”, which is one of the 5 types of splines/curves you can generate. Use Shift + A to bring up the dropdown menu for generating models and click “Curve >> Path”. This will generate the below object, which looks like arrows connected along the line (Green Box). Note: If you don’t see the arrows, click the Overlays icon (Red Box) and check the box at the bottom under “Curves”. This will generate the arrows for your eyes to see better. This line contains multiple vertices which will affect the direction of the spline.\\n\\nAssuming you’ve selected the object, click the ~ key (tilde) and change the view to Front. After that, click tab to switch to “Edit” mode. You should be able to see 5 vertices. Select any and start moving them up and down using the move tool on the left (AKA hotkey G on the keyboard). Congratulations, you’ve created used a Path to create a curve. See below for an example. The orange line represents the edited adjustments you’ve made to the curve, but the line itself is that represents the spline is represented by the black line.\\n\\nNow here are some things you can do with this curve.\\n\\nSelect an endpoint and press “E” on the keyboard. This is the extrude tool, which in this case, adds another vertex for you to use in your curve, allowing you to fine-tune the curve by adding more vertices/points into the spline.\\n\\nRefer to section 1. on the first page and you will see a yellow box in the right context menu. Assuming you have the spline object selected in “Object Mode”, this yellow box menu item titled “Object Data Properties” will be what you will use for spline object properties. In it, you will be able to better modify your spline to fit your modeling needs. Here are some of the things you can do with this menu:\\n\\nI. Object Data Properties >> Geometry >> Extrude. This allows you to extrude the spline itself, creating the image you see below. It basically expands the entire spline, essentially fattening it to an actual usable object.\\n\\nII. Object Data Properties >> Bevel >> Depth/Resolution. These tools allow you to expand the level of the spline, which adds and expands non-perpendicular surfaces out from the spline like an expanding, curved cylinder, as seen below. This is great for creating tunnels, whips, water slides, or other such objects.\\n\\nIII. Object Data Properties >> Active Spline >> Order U/Resolution U. Just by messing around with these tools, you can adjust the segmentation of the spline itself, creating sharper segments than the smooth curves we have been working with so far. See below for an example.\\n\\nWhat is a spline in blender?\\n\\nSplines are relatively simple. However, they have a lot of potential for creating unique objects that utilize curves in their structure. I know I haven’t gone through the use of constraints to lock objects onto a curved path yet, but that’s something we can discuss in future chapters. For now, just know that splines/curves are a tool in your arsenal that can help make modeling certain types of objects easier for you. Experiment around with the other types of curves in the Shift + A menu (like Beziers) to see whether those fit your modeling needs better. All of these curves can get the same result, but some may find using Beziers easier than using a Path for a particular purpose. In the end, experimentation is key and you’ll be able to fall back on splines whenever you need a curved war horn, whip animation, curved table leg, etc. in your future scenes.\\n\\nCategories Blender\\n\\nTags\\n\\nbest blender tutorials,\\n\\nblender tutorials,\\n\\nblender tutorials beginner,\\n\\neasy blender tutorials\\n\\nHow to Get Started on Flutter | For Both Non Programmers and Programmers\\n\\nLeave a Comment\\n\\nComment', metadata={'source': 'https://kickerai.com/blender-3d-tutorial-understanding-spline-modeling-in-blender/'}),\n",
              " Document(page_content='How to Add Lighting in Blender? Easy Blender 3D Tutorial\\n\\nAugust 20, 2023\\n\\nby Prasant\\n\\nHey Guys, In this tutorial, I will be discussing Lighting and its effect on Blender modeling.\\n\\nTo sum up, in general terms, these are a few of the ways that you may have seen how to light a scene:\\n\\nChapter 5: Blender Lighting\\n\\nCheck out Other Chapters:\\n\\nChapter 1: Getting Started with Blender: Introduction\\n\\nChapter 2: Things to Avoid When Starting Out\\n\\nChapter 3: 3D Blender Step-By-Step Tutorial: How to Create a Simple Vase in Blender?\\n\\nChapter 4: Understanding Spline Modeling in Blender\\n\\n1. World Properties:\\n\\nThe red box marks the menu needed to access World Properties.\\n\\nWorld properties affect lighting across the entire “world” and aren’t targeting a specific area. By changing the Color and Strength properties, you can essentially change the light color and brightness of the lighting of the entire 3d scene you are working in.\\n\\nThe “Volume” tab also creates lighting. By choosing Volume Scatter (pressing the green dot next to Volume), you can create a type of foggy effect, which if used correctly, can brighten up a scene with luminescent fog.\\n\\nRemember, this is a beginner teaching other beginners so don’t expect any technical jargon on why these things light up the scene the way they do. I’m merely presenting the guidelines for you all.\\n\\n2. HDRIs: High Dynamic Range Imaging\\n\\nThink of HDRIs as high-resolution images of backgrounds that generate lighting to the entire 3d world in a way that matches the image. If we have a forest HDRI for example, we would expect lighting similar to that of a forest (with patches of light coming through the trees and dimmer lighting in other areas where the trees block out the sunlight). There are two ways to access HDRIs (see below).\\n\\nThe first is by accessing the menu to the right of the “Viewport Shading” tab on the top right of the viewport. By turning off Scene Lights and Scene World, and turning up the “World Opacity” to the max, you can the HDRI image which generates the lighting information to the scene as well as see the changes to the lighting itself by clicking each of the orbs with images of backgrounds contained within it.\\n\\nThe second method of accessing HDRI is in “World Properties” again, but this time changing the “Color” to “Environment Texture” and choosing an HDRI file. To access this, click the yellow dot next to “Color”.\\n\\n3. Light Object: Point Light\\n\\nMost people get the default “Point” light in every Blender file that they start with. This is your standard lightbulb-esque light which can be turned up and down and affects the area around it in a spherical manner. It’s basically a circular light that emits lights in all directions. To access this light, you can just generate a new Object with Shift + A and select Light >> Point. The properties here include:\\n\\nColor: This affects the color of the light.\\n\\nPower: This affects how strong/bright the light is.\\n\\nDiffuse: This affects how scattered the light is, creating softer shadows (and also has an effect of darkening the overall lighting when lowered).\\n\\nSpecular: Affects the brightness of the light that reflects off of surfaces.\\n\\nVolume: Affects the brightness of the light within the volumetrics of the light. It’s a bit tricky. Think of using World properties to set up the foggy lighting effect I mentioned earlier. By increasing the volume for the point light in this “fog”, you are creating a stronger fog lighting near the point light then towards the surrounding area. See pictures below for examples.\\n\\nThe left has the volume turned up to max, and the right does not. Both have lighting, but the left has more “volume” of light.\\n\\nRadius: Radius affects how far the light reaches (think of an expanding sphere with the light represented by the sphere). The higher it is, the more likely it is that shadows will begin to soften (similar to diffuse but it doesn’t get darker in the process).\\n\\nShadow: Kind of self explanatory.\\n\\nContact Shadows: Shadows on the object itself that help highlight the object better. Think of a face with a light shining on it. The contact shadows will be the shadows that show up on the inside of a human ear, between the eyes and eyebrows, close to the nose, etc., which help highlight the ear, eyes, nose, etc.\\n\\n4. Light Object: Sun\\n\\nSun has similar properties to point except that it affects the entire scene, rather than just the area around the light. Think of the actual sun and how it lights things.\\n\\n5. Light Object: Spot\\n\\nSpot creates a spotlight. The properties are similar to the others but there is a “Spot Shape” Tab that allows you to change the size of the spotlight and the softness of the shadows around the spotlight (AKA how well the edges of the spotlight blend together with the shadow).\\n\\n6. Light Object: Area\\n\\nSimilar to a point light, an area light is a light that produces light from a surface but does not emit light in all directions the way a point light does. You can choose what kind of shape the area light will generate from, which include square, rectangle, disk, ellipse. You can also change the size of the object generating light by using Size X and Size Y (affecting the x and y-axis of the object).\\n\\n7. Emission Lighting:\\n\\nWithout going into the technical details, you can turn an object into a light source by first turning on the bloom effect in Render Properties.\\n\\nWithout bloom, you will not see the lights emitted through the emission shader on an object. After turning on Bloom, click the object and go to the materials tab at the bottom right, add a new material by clicking the “+\\xa0\\xa0 New” icon and scroll down until see “Surface” >> Emission. Change the color to whatever color you want the light source to be and then turn up the emission strength. This is only one way to turn up the emission and there are other methods, which we will not cover here.\\n\\nThis is just a general outline of the many ways you can add lighting to your scene. You can find more online tutorials on each of these methods in order to better refine the lighting that you use for a particular scene. Have fun blending!\\n\\nCheck out other Blender Related Articles:\\n\\n3D Blender Step-By-Step Tutorial: How to Create a Simple Vase in Blender?\\n\\nGetting Started with Blender: Introduction | Easy Tutorials For Beginners\\n\\nEasy Blender Tutorials: Things to Avoid when Starting out as a Beginner\\n\\nEasy Blender 3D Tutorial: Understanding Spline Modeling in Blender\\n\\nHow to Add Lighting in Blender? Easy Blender 3D Tutorial\\n\\nHow to Add Lighting in Blender Part 2?\\n\\nCategories Blender\\n\\nTags\\n\\nblender lighting,\\n\\nblender tutorials beginner,\\n\\nhow to add environmental lighting in blender,\\n\\nlighting in blender\\n\\nHow to Get Started on Flutter | For Both Non Programmers and Programmers\\n\\nLeave a Comment\\n\\nComment', metadata={'source': 'https://kickerai.com/how-to-add-lighting-in-blender-easy-blender-3d-tutorial/'}),\n",
              " Document(page_content='How to Create a Dialog Box in Flutter? DialogBox with Radio Button\\n\\nAugust 20, 2023\\n\\nby Prasant\\n\\nHello Everyone, In this article, I will be discussing how you can create a dialog box in Futter.\\n\\nDialog Box or Popup Box is important in Flutter. They are used to show additional information to the user and for other many purposes.\\n\\nThe dialog box you see in this article will be the popup box with the radio button as shown in the image below.\\n\\nIf you wish to know more about the dialog box and its type, then you can read this article by Tiya Chows on Medium.\\n\\nLet’s get started.\\n\\nHow to Create Popup in Flutter?\\n\\nWe will first create a New Project and run the default counter program code.\\n\\nNext, we will create a new dart file called popup.dart. Inside this file, we will write the code for the dialog box.\\n\\nHere, We created a StatefulWidget named PopupWidget. Then we returned the Dialog widget and defined the shape of the Dialog box using the shape property. The shape will be the Rounded Rectangle with a 16 border radius.\\n\\nTo test, we called the PopupWidget in the floatingactionbutton of main.dart file.\\n\\nWhen we ran the program, we got the following output as in the image below.\\n\\nThe Next Step will be to add the Filter text and close icon button as shown in the image below.\\n\\nFor this, Inside the Column Widget, we are using Row Widget. Then with the below code, we can get the Filter text and orange close button.\\n\\nFinally, it’s time to add the radio buttons. For this, I followed the official flutter documentation on the radio button. If anyone wants to check out, here’s the link: Flutter Radio Buttons Documentation\\n\\nThe documentation used the ListTile widget for the radio button, so I am also using ListTile in this program. If you wish to know more about the function of properties such as trailing, value, activeColor, and more, study the documentation.\\n\\nWe are defining the items using enum.\\n\\nAfter adding 4 items in the Column of PopupWidget, finally, we created the dialog box.\\n\\nNow we have a customized popup dialog box with radio buttons.\\n\\nI hope this article helped you to learn how you can create the dialog box in a flutter. Thanks for reading.\\n\\nCheck out other articles:\\n\\nLogin and Register Easily with Flutter using Firebase\\n\\nHow to Create Tic Tac Toe in Flutter?\\n\\nHow to Fetch Data from Internet? Flutter Tutorial?\\n\\nCommon Flutter Errors and Solution\\n\\nFull Code:\\n\\nmain.dart\\n\\npopup.dart\\n\\nCategories Flutter\\n\\nTags\\n\\nflutter custom popup,\\n\\nflutter dialog box,\\n\\nflutter dialog box pop up,\\n\\nflutter popup box\\n\\nHow to Get Started on Flutter | For Both Non Programmers and Programmers\\n\\nLeave a Comment\\n\\nComment', metadata={'source': 'https://kickerai.com/how-to-create-a-dialog-box-in-flutter-dialogbox-with-radio-button/'}),\n",
              " Document(page_content='How to Add Lighting in Blender Part 2?\\n\\nAugust 20, 2023\\n\\nby Prasant\\n\\nChapter 5.5: Lighting (Bonus)\\n\\nHey everyone. It’s been a bit busy for me this week, but I didn’t want to leave you guys hanging so I’m adding a short two-page bonus lesson on how you should set up a basic scene for lighting.\\n\\nChapter 1: Getting Started with Blender: Introduction\\n\\nChapter 2: Things to Avoid When Starting Out\\n\\nChapter 3: 3D Blender Step-By-Step Tutorial: How to Create a Simple Vase in Blender?\\n\\nChapter 4: Understanding Spline Modeling in Blender\\n\\nChapter 5: How to Add Lighting in Blender Part 1?\\n\\nIn short, we’re going to be learning about 3-point lighting, and assuming that you didn’t know about this already, three-point lighting is the standard lighting setup used for most film sequences and includes:\\n\\n1. Key Light:\\n\\nThis is the main light that shines upon the focal point in a scene, whether it be an object or a character. It is the light that highlights the scene surrounding a character or an object in order to make it stand out and become the focus of a scene. Normally, you do NOT want to implement a key light facing directly at an object, because that would rid the object of all its shadows and take away the emotion from a scene. Lighting and contrast are meant to tell a story.\\n\\n2. Fill light:\\n\\nThis is a lighting whose purpose is to reduce the stark contrast you would get from shadows in order to portray a more natural-looking shot of an object or person (unless you are going for something extremely stylistic). It is a good tool for simulating bounce lighting (AKA lighting that reflects off walls) and helps make things look a little more realistically grounded, without taking away from the key light. Fill lights are usually placed lower than key lights, to the opposite direction of the key light, and are not as bright as key lighting is.\\n\\n3. Rim light/backlight:\\n\\nThis is essentially silhouette lighting which highlights the shape of the character/object and is usually placed behind and above the character/object. Its purpose is to use the lighting to create a thin outline of the character and emphasize the contours of the character/object. (For the below example I used a darker background to emphasize the rim light’s effect).\\n\\nLastly, if you want to set your scene prior to adding your lights, you might want to look into creating an infinity room/background or a beveled plane. These will help create the illusion of an infinite room and help you set up the backdrop for your object prior to adding lighting, such as the one used in the last photo above.\\n\\nIf you want to get good at Blender, Consider Taking the course below:\\n\\nBlender Guru is good at explaining how to set up this beveled plane:\\n\\nInfinity background using gradient nodes:\\n\\nInfinity background using objects:\\n\\nGood luck everyone!\\n\\nCategories Blender\\n\\nTags add lighting in blender, blender tutorials beginner\\n\\nHow to Get Started on Flutter | For Both Non Programmers and Programmers\\n\\nLeave a Comment\\n\\nComment', metadata={'source': 'https://kickerai.com/add-lighting-in-blender-part-2/'}),\n",
              " Document(page_content='Create a Simple Dialog Box in Flutter\\n\\nAugust 20, 2023\\n\\nby Prasant\\n\\nIn this article, I will post the code on how you can create a Simple Dialog Box in Flutter.\\n\\nThe dialog box we will be creating is shown in the image below.\\n\\nAfter Clicking on a button, ShowDialog() is triggered and the dialog box as shown in the image above is shown on the screen.\\n\\nThe shape of the dialog box is defined by using shape. Here we have given the shape of RoundedRectangleBorder with a border radius of 16.0 to all of its sides.\\n\\nRelated: Create a Dialog Box with Radio Buttons\\n\\nA TextEditingController addNewFeatureController( in Screen.dart) is defined to get the text User enters in the dialog box. addNewFeatureController.text gives the text user entered.\\n\\nThe Source Code below shows the code to create a simple dialog box in Flutter.\\n\\nScreen.dart\\n\\ndialog_widget.dart\\n\\nRelated Posts:\\n\\n[Solved] Don’t use one refreshController to multiple SmartRefresher\\n\\nWhy isn’t Pubsec in dart?\\n\\nUnwrap Key Failed Flutter Error\\n\\nLogin and Register Easily with Flutter using Firebase\\n\\nInternet Connection Checker Using Bloc [ Flutter Code Example ]\\n\\nCategories Flutter\\n\\nTags create a dialog box in flutter, flutter for beginners\\n\\nHow to Get Started on Flutter | For Both Non Programmers and Programmers\\n\\nLeave a Comment\\n\\nComment', metadata={'source': 'https://kickerai.com/create-a-simple-dialog-box-in-flutter/'}),\n",
              " Document(page_content='Flutter Rounded Container Code Example\\n\\nAugust 20, 2023\\n\\nby Prasant\\n\\nHow to Make a Rounded Container in Flutter?\\n\\nRelated:\\n\\nCreate a Dialog Box in Flutter with Radio Buttons\\n\\nHow to Fetch Data from Internet? Flutter Tutorial\\n\\nLogin and Register Easily with Flutter using Firebase\\n\\nCategories Flutter\\n\\nHow to Get Started on Flutter | For Both Non Programmers and Programmers\\n\\nLeave a Comment\\n\\nComment', metadata={'source': 'https://kickerai.com/flutter-rounded-container-code-example/'}),\n",
              " Document(page_content=\"Flutter Errors and Solution\\n\\nAugust 20, 2023\\n\\nby Prasant\\n\\nIn this post, I will be discussing errors in Flutter and their Possible Solution.\\n\\nThe errors in this list are those errors I personally came across and these solutions were able to solve those errors.\\n\\nTable of Contents:\\n\\nGeoLocator Package Giving No Output\\n\\nBottom Overflow when Keyboard Appears\\n\\nFutter Navigation “Context is Undefined”\\n\\nsetState() or markNeedsBuild() called during build.\\n\\n1. GeoLocator Package Giving No Output\\n\\nGeoLocator package enables us to get the users’ location and many other features. The thousands of lines of codes are readily available for us once we add the GeoLocator package to our project.\\n\\nWhat to do when Geolocator Package gives you no output?\\n\\nTwo possible Solutions:\\n\\n1. Turn on the Location of the device manually. Check if the device is getting Location Permission. If not, give the Permission Manually.\\n\\n2. Sometimes,\\n\\ndesiredAccuracy:\\xa0LocationAccuracy.Low or\\n\\ndesiredAccuracy:\\xa0LocationAccuracy.Lowest does not give any output. So use\\n\\ndesiredAccuracy:\\xa0LocationAccuracy.high\\n\\n2. Bottom Overflow when Keyboard Appears\\n\\nHow to Solve Flutter Bottom Overflow when Keyboard Appears?\\n\\nThere are two quick solutions to solve bottom overflow when the keyboard appears in Flutter.\\n\\n1. Use\\n\\nresizeToAvoidBottomInset: false under Scaffold\\n\\n2. Use\\n\\nSingleChildScrollView. Click here to go to its\\n\\ndocumentation.\\n\\n3. Flutter Navigation “Context is undefined”\\n\\nmoreInfo(){\\n  return GestureDetector(\\n    child: Text('More Info'),\\n    onTap: (){\\n      Navigator.push(context, MaterialPageRoute(builder: (context) => UserComments()));\\n    }\\n    );\\n}\\n\\nHow to solve Flutter Navigation “Context is undefined” error?\\n\\nThe Navigator’s context needs BuildContext for navigation otherwise it will just give us “Context is undefined error” just like the error shown in the image.\\n\\nThe best solution would be to use Builder Class. Here’s the link to its documentation: Builder Class. The code on how to use BuilderClass is shown below.\\n\\nAnother solution would be just to pass the Buildcontext.\\n\\nmoreInfo() {\\n  return Builder(builder: (BuildContext context) {\\n    return GestureDetector(\\n      child: Text('More Info'),\\n      onTap: () {\\n        Navigator.push(\\n            context, MaterialPageRoute(builder: (context) => UserComments()));\\n      },\\n    );\\n  });\\n}\\n\\n4. setState() or markNeedsBuild() called during build.\\n\\nWhat to do when you get “setState() or markNeedsBuild() called during build.”?\\n\\nNot really sure what other times, one can get this error, but I got it when I tried to call a function _tapped on onTap like this onTap: _tapped(). The solution which took me hours to get was to just remove the brackets from _tapped(). The final solution was onTap: _tapped\\n\\n5. How Flutter Error: Vertical viewport was given unbounded height Occurs ?\\n\\nI was gonna write about why this error occurs and possible solution. However, I found a good youtube video which is by flutter team which explains about this problem.\\n\\n5. Null check operator used on a null value\\n\\nThe solution is to add onChanged: (_) {} or onCompleted: (){}. This is because onChanged or onCompleted will  working even if we don’t need them.\\n\\nMore info on: OTP TextField Issues\\n\\n6. Chip Doesn’t Move Down\\n\\nUse ListView.generate Instead of ListView.builder With WRAP Widget\\n\\nCategories Flutter\\n\\nTags flutter errors, flutter solution\\n\\nHow to Get Started on Flutter | For Both Non Programmers and Programmers\", metadata={'source': 'https://kickerai.com/flutter-errors-and-solution/'}),\n",
              " Document(page_content='Internet Connection Checker Using Bloc [ Flutter Code Example ]\\n\\nAugust 20, 2023\\n\\nby Prasant\\n\\nHere in this article, I will be discussing how you can check the internet connection status using Flutter Bloc and Connectivity Plus Package. You can get both the package on Pub.dev\\n\\nI will also be discussing how the events and states will work in simple terms based on my understanding.\\n\\nIn this code, I am using the latest version of the Bloc which is version 8.\\n\\nFeel Free to Ask the Question in the Comments if you don’t understand the code.\\n\\nHere’s the GitHub repo: Internet Connection Checker\\n\\nHow to check the internet connection continuously in Flutter?\\n\\nconnected_event.dart\\n\\nEvents are generally triggered by the users. Here the user either turns off the wifi or turns on the wifi. So We have two events mainly. One OnConnectedEvent where users turn on the wifi. Another OnNotConnectedEvent where the user turns off the wifi or mobile internet.\\n\\nconnected_state.dart\\n\\nStates are generally the results of the events. Here I have defined three internet connection states. One state/possibility can be the Initial State ConnectedInitialState where nothing is happening. The second possibility is We are successfully connected to the internet ConnectedSuccessState. The last possibility is we are not connected to the internet ConnectedFailureState. Here connection both includes mobile data and wifi.\\n\\nconnected_bloc.dart\\n\\nYou can define your custom states and events, names really don’t matter. What matters is that you have an understanding of how events are triggered and which events are being triggered and as a result which state appears. For example, We know that once the user turns off the wifi, the event we define earlier OnNotConnectedEvent is triggered and as a result, we get a state where we have no internet connection ConnectedFailureState.\\n\\nThis is what the code represents in the connected_bloc.dart.\\n\\non<OnConnectedEvent>((event, emit) => emit(ConnectedSucessState()));\\n    on<OnNotConnectedEvent>((event, emit) => emit(ConnectedFailureState()));\\n\\nThis means if the OnConnectedEvent event is triggered, emit (produce) ConnectedSuccessState.\\n\\nThe Next Step is to define when OnConnectedEvent should be triggered/happen. As we know that the event should happen when the user disconnects this mobile or wifi connection. We have to detect that using the Connectivity Plus Package. How do we do that? It is clearly explained in the documentation of that package.\\n\\nif (result == ConnectivityResult.mobile ||\\n          result == ConnectivityResult.wifi) {\\n        add(OnConnectedEvent());\\n      } else {\\n        add(OnNotConnectedEvent());\\n      }\\n\\nIn short, If the result is Wifi Connection or Mobile Data Connection, the OnConnectedEvent event is triggered.\\n\\nHomePage.dart\\n\\nHere we are using Bloc Consumer which provides us with Bloc Listener and Bloc Builder. You can either use one of them. I am using both as an example. Bloc Listener simply listens and doesn’t change make any change in UI. Bloc Builder changes the UI.\\n\\nIf we turn off the wifi, the state changes to ConnectedFailureState.As a result, A Scaffold Messenger will appear (Bloc Listener) and the UI will change and will show to show “Not Connected” text in the Center as defined in the Bloc Builder method above.\\n\\nSimply,\\n\\nThe user turns off the wifi —–> Stream Listens ——-> OnNotConnectedEvent event triggered ——-> State is ConnectedFailureState ——> Change in Device as defined\\n\\nHope you understood how the whole things work in the internet connection check using Flutter Bloc and Connectivity Plus Package. It is also possible to check the internet simply using the Connectivity Plus Package alone. It is easier that way if you just want to show the message/notification of the internet status.\\n\\nOnce again, feel free to comment if you don’t understand anything. Thanks for reading. Have a good day.\\n\\nHere’s the GitHub repo: Internet Connection Checker\\n\\nRelated Flutter Posts\\n\\nCreate a Simple Dialog Box in Flutter\\n\\nCreate Games with Flutter | Tic Tac Toe\\n\\nFlutter Error: The Android Gradle plugin supports only kotlin-android-extensions Gradle plugin version 1.6.20 and higher\\n\\nFlutter Errors and Solution\\n\\nFlutter Rounded Container Code Example\\n\\nCategories Flutter\\n\\nTags\\n\\ncheck internet using flutter bloc,\\n\\nflutter bloc,\\n\\nfutter code example,\\n\\ninternet connection checker flutter bloc\\n\\nHow to Get Started on Flutter | For Both Non Programmers and Programmers\\n\\n5 thoughts on “Internet Connection Checker Using Bloc [ Flutter Code Example ]”\\n\\nOch\\t\\t\\t\\t\\t\\t\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tMay 1, 2022 at 1:07 pm\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\n\\t\\t\\t\\t\\n\\t\\t\\t\\t\\tI followed these steps, but not working. Why don’t you put the project into git?\\n\\nPrasant\\t\\t\\t\\t\\t\\t\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tMay 5, 2022 at 8:08 am\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\n\\t\\t\\t\\t\\n\\t\\t\\t\\t\\tWhat exactly went wrong? It was a simple project with a few lines of code so I didn’t put it on GitHub. You can ask here your problem.\\n\\nPranay\\t\\t\\t\\t\\t\\t\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tMay 16, 2022 at 7:22 am\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\n\\t\\t\\t\\t\\n\\t\\t\\t\\t\\tThe following ProviderNotFoundException was thrown building RawGestureDetector-[LabeledGlobalKey#fac77](state: RawGestureDetectorState#cf147(gestures: , behavior: opaque)):\\n\\nPrasant\\t\\t\\t\\t\\t\\t\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tMay 16, 2022 at 4:48 pm\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\n\\t\\t\\t\\t\\n\\t\\t\\t\\t\\tHere I have put the project on the GitHub. Hope it helps.\\n\\nPrasant\\t\\t\\t\\t\\t\\t\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tMay 16, 2022 at 4:48 pm\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\n\\t\\t\\t\\t\\n\\t\\t\\t\\t\\tI have put the project on the github. Sorry for being late.\\n\\nComments are closed.', metadata={'source': 'https://kickerai.com/internet-connection-checker-using-bloc-flutter-code-example/'}),\n",
              " Document(page_content='Face Recognition Attendance System\\n\\nAugust 20, 2023\\n\\nby Prabhat\\n\\nFace Recognition Attendance System is an attendance system based on face recognition which automatically marks the attendance of the present employee on recognizing the face of an employee. Over the past years, attendance was done by calling the names or by signing the signature on the attendance paper. As time gradually passes the new way of keeping attendance records of staff has been shifted to the technology that uses biometric which requires eye, face, or fingerprint as user input. Among all other biometrics, Face recognition is an ongoing research topic in the field of computer vision and is widely adopted due to its contactless process.\\n\\nEvery day, we look into\\xa0our phones thousands of times and end up using face recognition technology for unlocking our phones. Facebook uses its own face recognition algorithm for recognizing you and your friends. China has created a vast surveillance system equipped with facial recognition technology to monitor its citizens. Similarly, the FBI uses facial recognition to spot criminals.\\n\\nBut the question is\\xa0“Have you ever taken a closer look at how face recognition works?”\\xa0If no, don’t worry. In this post we are going to reveal how it works and build one for us for automating the attendance process.\\n\\nSo, Let’s get started with the introduction….\\n\\nTable of Contents:\\n\\nWhat is Face Recognition?\\n\\nHow Face Recognition Works?\\n\\nInspiration For Building This System\\n\\nSmart Attendance SystemSmart Attendance System Project StructureHow Smart Attendance System Works?Registration PhaseTraining PhaseExtraction Of Face EmbeddingsTraining SVM AlgorithmAttendance Updation\\n\\nFAQ\\n\\nLooking for the source code to this post? Get the code on Github\\n\\nWhat is Face Recognition?\\n\\nFace Recognition\\xa0is a method of identifying an individual by comparing their faces with the stored faces in the database. Imagine you own an Iphone and you want to use Faceid to unlock your phone. First, you need to setup your faceid by capturing your face image at all possible angles. Then the system stores those faces in a database and finally, it recognizes you and unlocks your phone by comparing your faces with the stored faces in the database.\\n\\nAfter understanding the definition of face recognition, now the questions may arise how this technology works and how is it able to recognize the face belonging to a particular person. For knowing things in detail, let’s dive deep into the algorithm used in the face recognition process.\\n\\nHow Face Recognition Works?\\n\\nWell, face recognition systems are in the rapid development phase and are accumulated with a new strong algorithm that improves the system day by day. With advancements in deep learning and computer vision, face recognition technology has gained much popularity over the years. Many researchers have come out with new deep learning architectures to significantly improve the performance of face recognition algorithms. However, among all other algorithms, FaceNet is one of them which achieves state-of-the-art results on a range of face recognition benchmark datasets.\\n\\nFaceNet is the name of the facial recognition algorithm that was proposed by Google Researchers in 2015 in the paper titled“ FaceNet: A Unified Embedding for Face Recognition and Clustering”. They proposed an approach in which it maps each face image into 128-d feature vectors such that the image of the person will be placed closer to all other images of the same person as compared to images of any other person present in the dataset. The main difference between FaceNet and other techniques is that it learns the mapping from the images and creates embeddings rather than using any bottleneck layer for recognition or verification tasks. It used a method called triplet loss as a loss function to do so. Let’s take a look at the architecture of FaceNet algorithm.\\n\\nThese deep learning models output a 128 dimensions embedding of an image with\\xa0L2\\xa0normalization performed on it. These embeddings are then passed into the loss function to calculate the loss. The goal of this loss function is to make the squared distance between two embeddings of the same identity small, whereas the squared distance between two embeddings of different identity large. Therefore a new loss function called\\xa0Triplet loss\\xa0is used.\\n\\nThe intuition behind the triplet loss function is that we want our anchor image(image of a specific person A) to be closer to positive images(all the images of person A) as compared to negative images (all other images). In other words, we can say that we want the distances between the embedding of our anchor image and the embeddings of our positive images to be lesser as compared to the distances between embeddings of our anchor image and embeddings of our negative images.\\n\\nTriplet loss function can be formally defined as:\\n\\nHere f(x) takes x as an input and returns a 128-dimensional vector, i denotes i’th input and the subscript a denotes Anchor image, p denotes Positive image and n denotes Negative image.\\n\\nNow, we get an idea of how face recognition works. So, let’s give it a try to build a system to automate the attendance process using face recognition.\\n\\nInspiration For Building This System\\n\\nTraditional methods of attendance are\\xa0time and effort-consuming. In big organizations where many staff are employed, it’s hard to perform attendance manually and there may be chances of misplacement of attendance sheets leading to inaccurate and inefficient attendance. Employees can easily get rid of time recording options by committing\\xa0time theft\\xa0in manual-based attendance systems. So, by observing these kinds of problems, I got inspired to build a system\\xa0“Smart Attendance System”\\xa0which not only automates the attendance process but also prevents the employee from committing time thefts.\\n\\nSmart Attendance System\\n\\nSmart Attendance System\\xa0is a system that automatically marks the attendance of a present employee by recognizing the face of an employee. Besides that, it also records the date and time at which the employee interacts with the system.\\n\\nFor a complete understanding of how the Smart Attendance System works, see this video.\\n\\nhttps://www.youtube.com/watch?v=BECN0Cn7UAc&t=215s\\n\\nSmart Attendance System Project Structure\\n\\nOur system has three top level directories :\\n\\nAttendance_Details/ :\\xa0It is an empty directory at the beginning and finally the attendance report is stored in .csv format inside this directory after attendance marking.\\n\\ndataset/ :\\xa0Contains face images for two subjects organized into subdirectories based on their respective names. Each subdirectory contains 50 face images. New subdirectory is added based on the name provided by the user as an input during the registration process and their 50 portrait faces are stored inside that subdirectory.\\n\\nmodels/ :\\xa0contains pretrained model files for face detection algorithm and face recognition algorithm. ‘facenet_keras.h5’ is a pretrained facenet model whereas other is the opencv’s haarcascade classifier.\\n\\nWe also have five main files in the root directory:\\n\\nregistration.py\\xa0: takes the user portrait face and stores them in dataset directory based on the name provided by the user.\\n\\nextract_embeddings.py\\xa0: 128-d facial embeddings are created with this script.\\n\\ntraining.py\\xa0: training script to create a recognizer model by training SVM algorithm on feature vectors and labels.\\n\\nmark_attendance.py\\xa0: script to create a csv file which can store date, name, time and attendance status of an employee.\\n\\nrecognize.py\\xa0: recognizes the identity of staff and automatically mark their attendance as present.\\n\\nHow Smart Attendance System Works?\\n\\nSmart Attendance System had to go through three phases :- Registration phase, Training phase and Testing phase.\\n\\nIn the registration phase, the system had to take in user inputs and make use of a camera sensor i.e webcam to gather the training datasets. The camera accepts the facial inputs from the staff and the Viola-Jones algorithm detects the facial region from the captured frames of each staff. Then the detected faces are stored in a separate folder which makes the training datasets which completes our registration process.\\n\\nThe training is done by using a two way process. In the first process, the facial embeddings are extracted from a FaceNet model which uses triplet loss to output a 128-d feature vector called an embedding. Then, in the second process, we train a SVM classifier to a list of embeddings to create a recognition model.\\n\\nIn the testing phase, the person faces the webcamera for doing their attendance. The system detects the face, crops the facial region and resized the face image into 160 * 160 pixel. Then the resized face is provided as an input to the facenet model which outputs a 128-d feature vectors. Then this 128-d feature vector is provided as an input to the recognizer model which recognizes the identity of that person and mark their attendance as present.\\n\\nThe whole system is divided into following stages as given below:-\\n\\n1. Registration Phase\\n\\nThis is the first phase in our system in which the registration of an employee takes place. The steps involved in the registration phase are as follows:\\n\\nThe user inputs his full name through the command line interface.\\n\\nInitialization of web camera : After taking user inputs, the web camera is intialized.\\n\\nPortrait acquistion of user : Then the portrait acquisition of each user is done by using a web camera to capture 50 images.\\n\\nFace Detection : On each captured image, we apply face detection algorithm and draw a rectangular bounding box at the detected facial region. Here, we have used OpenCV haarcascade classifier for face detection.\\n\\nImage Resizing : After face detection, the detected facial region are extracted and then resized into 160 * 160 pixels because the FaceNet model requires the input dimensions to be 160 * 160 pixels.\\n\\nStoring images in a local directory : Finally, those resized face images are stored in a local database under the directory with the name of a user.\\n\\nThis completes the registration phase and here is the diagram representing the image acquistion and the registration process:\\n\\nHere is the working code sample written in Python that performs all the steps necessary for registering the user and storing their images under the directory with the name of a user.\\n\\nregister.py\\n\\nimport cv2\\nimport numpy as np\\nimport os\\nface_cascade = cv2.CascadeClassifier(\"models/haarcascade_frontalface_default.xml\")\\nroot_dir = os.getcwd()\\ndataset_dir = os.path.join(root_dir,\\'dataset\\')\\nname = input(\"Enter your name: \")\\ninput_directory = os.path.join(dataset_dir,name)\\nif not os.path.exists(input_directory):\\n    os.makedirs(input_directory, exist_ok = \\'True\\')\\n    count = 1\\n    print(\"[INFO] starting video stream...\")\\n    video_capture = cv2.VideoCapture(0)\\n    while count <= 50:\\n        try:\\n            check, frame = video_capture.read()\\n            gray = cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY)\\n            faces = face_cascade.detectMultiScale(gray,1.3,5)\\n            for (x,y,w,h) in faces:  \\n                face = frame[y-5:y+h+5,x-5:x+w+5]\\n                resized_face = cv2.resize(face,(160,160))\\n                cv2.imwrite(os.path.join(input_directory,name + str(count) + \\'.jpg\\'),resized_face)\\n                cv2.rectangle(frame, (x,y), (x+w, y+h),(0,0,255), 2)\\n                count += 1\\n            # show the output frame\\n            cv2.imshow(\"Frame\",frame)\\n            key = cv2.waitKey(1)\\n            if key == ord(\\'q\\'):\\n                break\\n        except Exception as e:\\n            pass\\n    video_capture.release()\\n    cv2.destroyAllWindows()\\nelse:\\n    print(\"Photo already added for this user..Please provide another name for storing datasets\")\\n\\nFor running this file, you just go to the terminal, and then use the following command:\\n\\npython register.py\\n\\n2. Training Phase\\n\\nIn training phase, the training datasets is provided as inputs to the pretrained facenet model for extracting a list of 128-d feature vectors called facial embeddings. Here,a list of 128-d feature vectors act as features and the names of person act as a label. Then we train a machine learning algorithm by using supervised machine learning approach on those features and labels to create a recognizer model. The training phase is divided into further two stages:-\\n\\n2.1 Extraction Of Face Embeddings\\n\\nAfter the creation of a facial database, the training datasets are provided as inputs to the pretrained FaceNet model for extracting a list of 128-d feature vectors called embeddings. The distance between 128-d feature vectors of the same person is smaller whereas this distance is larger for faces of different persons. The pretrained weight file of a FaceNet model is available under the ‘models’ directory. After the embeddings are extracted successfully, we serialized 128-d feature vectors along with their associated class name in our embeddings model and saved the serialized format to a file named ‘embeddings.pickle’ under the models directory.\\n\\nmodels\\n\\n|—– facenet.h5\\n\\n| — — — embeddings.pickle\\n\\nHere, ‘facenet.h5’ is a pretrained FaceNet model and ‘embeddings.pickle’ is a pickle object of our serialized embeddings model.\\n\\nThe below code sample accepts the resized training images as an input which is then forwarded to the facenet model for extracting 128-d feature vectors called labels.\\n\\nextract_embeddings.py\\n\\nimport cv2\\nimport os\\nimport numpy as np\\nfrom tensorflow.keras.models import load_model\\nimport pickle\\nrootdir = os.getcwd()\\ndataset_dir = os.path.join(rootdir,\\'dataset\\')\\nmodel_path = os.path.join(rootdir,\\'models/facenet_keras.h5\\')\\nfacenet_model = load_model(model_path)\\ncategories = os.listdir(dataset_dir)\\ndef check_pretrained_file(embeddings_model):\\n\\tdata = pickle.loads(open(embeddings_model,\"rb\").read())\\n\\tnames = np.array(data[\"names\"])\\n\\tunique_names = np.unique(names).tolist()\\n\\treturn [data,unique_names]\\ndef get_remaining_names(unique_names):\\n\\tremaining_names = np.setdiff1d(categories,unique_names).tolist()\\n\\treturn remaining_names\\ndef get_all_face_pixels():\\n\\timage_ids = []\\n\\timage_paths = []\\n\\timage_arrays = []\\n\\tnames = []\\n\\tfor category in categories:\\n\\t\\tpath = os.path.join(dataset_dir,category)\\n\\t\\tfor img in os.listdir(path):\\n\\t\\t\\timg_array = cv2.imread(os.path.join(path,img))\\n\\t\\t\\timage_paths.append(os.path.join(path,img))\\n\\t\\t\\timage_ids.append(img)\\n\\t\\t\\timage_arrays.append(img_array)\\n\\t\\t\\tnames.append(category)\\n\\treturn [image_ids,image_paths,image_arrays,names]\\ndef get_remaining_face_pixels(remaining_names):\\n\\timage_ids = []\\n\\timage_paths = []\\n\\timage_arrays = []\\n\\tnames = []\\n\\tface_ids = []\\n\\tif len(remaining_names) != 0:\\t\\n\\t\\tfor category in remaining_names:\\n\\t\\t\\tpath = os.path.join(dataset_dir,category)\\n\\t\\t\\tfor img in os.listdir(path):\\n\\t\\t\\t\\timg_array = cv2.imread(os.path.join(path,img))\\n\\t\\t\\t\\timage_paths.append(os.path.join(path,img))\\n\\t\\t\\t\\timage_ids.append(img)\\n\\t\\t\\t\\timage_arrays.append(img_array)\\n\\t\\t\\t\\tnames.append(category)\\n\\t\\treturn [image_ids,image_paths,image_arrays,names]\\n\\telse:\\n\\t\\treturn None\\ndef normalize_pixels(imagearrays):\\n\\tface_pixels = np.array(imagearrays)\\n\\t# scale pixel values\\n\\tface_pixels = face_pixels.astype(\\'float32\\')\\n\\t# standardize pixel values across channels (global)\\n\\tmean, std = face_pixels.mean(), face_pixels.std()\\n\\tface_pixels = (face_pixels - mean) / std\\n\\treturn face_pixels\\nembeddings_model_file = os.path.join(rootdir,\"models/embeddings.pickle\")\\nif not os.path.exists(embeddings_model_file):\\n\\t[image_ids,image_paths,image_arrays,names] = get_all_face_pixels()\\n\\tface_pixels = normalize_pixels(imagearrays = image_arrays)   \\n\\tembeddings = []\\n\\tfor (i,face_pixel) in enumerate(face_pixels):\\n\\t\\tsample = np.expand_dims(face_pixel,axis=0)\\n\\t\\tembedding = facenet_model.predict(sample)\\n\\t\\tnew_embedding = embedding.reshape(-1)\\n\\t\\tembeddings.append(new_embedding)\\n\\t\\tdata = {\"paths\":image_paths, \"names\":names,\"imageIDs\":image_ids,\"embeddings\":embeddings}\\n\\tf = open(\\'models/embeddings.pickle\\' , \"wb\")\\n\\tf.write(pickle.dumps(data))\\n\\tf.close()\\nelse:\\n\\t[old_data,unique_names] = check_pretrained_file(embeddings_model_file)\\n\\tremaining_names = get_remaining_names(unique_names)\\n\\tdata = get_remaining_face_pixels(remaining_names)\\n\\tif data != None:\\n\\t\\t[image_ids,image_paths,image_arrays,names] = data\\n\\t\\tface_pixels = normalize_pixels(imagearrays = image_arrays)\\n\\t\\tembeddings = []\\n\\t\\tfor (i,face_pixel) in enumerate(face_pixels):\\n\\t\\t\\tsample = np.expand_dims(face_pixel,axis=0)\\n\\t\\t\\tembedding = facenet_model.predict(sample)\\n\\t\\t\\tnew_embedding = embedding.reshape(-1)\\n\\t\\t\\tembeddings.append(new_embedding)\\n\\t\\tnew_data = {\"paths\":image_paths, \"names\":names,\"imageIDs\":image_ids,\"embeddings\":embeddings}\\n\\t\\tcombined_data = {\"paths\":[],\"names\":[],\"face_ids\":[],\"imageIDs\":[],\"embeddings\":[]}\\n\\t\\tcombined_data[\"paths\"] = old_data[\"paths\"] + new_data[\"paths\"]\\n\\t\\tcombined_data[\"names\"] = old_data[\"names\"] + new_data[\"names\"]\\n\\t\\tcombined_data[\"face_ids\"] = old_data[\"face_ids\"] + new_data[\"face_ids\"]\\n\\t\\tcombined_data[\"imageIDs\"] = old_data[\"imageIDs\"] + new_data[\"imageIDs\"]\\n\\t\\tcombined_data[\"embeddings\"] = old_data[\"embeddings\"] + new_data[\"embeddings\"]\\n\\t\\tf = open(\\'models/embeddings.pickle\\' , \"wb\")\\n\\t\\tf.write(pickle.dumps(combined_data))\\n\\t\\tf.close()\\n\\telse:\\n\\t\\tprint(\"No new data found... Embeddings has already extracted for this user\")\\n\\n2.2 Training SVM Algorithm\\n\\nAfter the extraction of embeddings, the list of embeddings are trained by using a SVM algorithm to create a recognition model. First, we need to load ‘embeddings.pickle’ file and deserialize it to get a list of embeddings and a list of classes associated with each embedding. Then a list of embeddings are treated as features and a list of classes are treated as labels. Then we train a SVM algorithm on known features and labels to create a model that can predict a label when given a 128-d feature vector. After the training is completed, a recognizer model is created which is serialized and saves the serialized format to a file named ‘recognizer.pickle’ under the models directory. Later we can load this file and deserialize this model and use it during the testing phase for automatically marking the attendance.\\n\\nmodels\\n\\n| — — — recognizer.pickle\\n\\ntrain.py\\n\\n\"\"\"\\nTrain ML Model to Classify / Identify the person using extracted face embeddings\\n\"\"\"\\nfrom sklearn.preprocessing import LabelEncoder\\nfrom sklearn.svm import LinearSVC\\nimport pickle\\nimport numpy as np\\nimport os\\nfrom sklearn.calibration import CalibratedClassifierCV\\nrootdir = os.getcwd()\\nembeddings_path = os.path.join(rootdir,\\'models/embeddings.pickle\\')\\ndef load_embeddings_and_labels():\\n    data = pickle.loads(open(embeddings_path, \"rb\").read())\\n    # encoding labels by names\\n    label = LabelEncoder()\\n    names = np.array(data[\"names\"])                       \\n    labels = label.fit_transform(names)\\n    # getting names\\n    # getting embeddings\\n    Embeddings = np.array(data[\"embeddings\"])\\n    return [label,labels,Embeddings,names]\\ndef create_svm_model(labels,embeddings):\\n    model_svc = LinearSVC()\\n    recognizer = CalibratedClassifierCV(model_svc)   \\n    recognizer.fit(embeddings,labels)\\n    return recognizer\\n[label,labels,Embeddings,names] = load_embeddings_and_labels()\\nrecognizer = create_svm_model(labels=labels,embeddings=Embeddings)\\nf1 = open(\\'models/recognizer.pickle\\', \"wb\")\\nf1.write(pickle.dumps(recognizer))\\nf1.close()\\nprint(\"Training done successfully\")\\n\\n3. Attendance Updation\\n\\nAfter the face recognition process, the face of staff is recognized and their attendance is marked in the attendance report and the rest will be marked as absent. The list of absentees will be mailed to the respective staff. The attendance sheet is maintained in a CSV format where the name of staff, date, and their attendance status are recorded along with the timestamp of their attendance period. The attendance taking time interval is fixed (say between 10 to 11 am) and the staff must mark their attendance during that time interval. If the staff are unable to mark their attendance during that time interval, their attendance status is marked as absent and an email notification is sent to them regarding their attendance status. The attendance sheet is automatically generated by the system which is named ‘attendance.csv’ and stored under the ‘reports’ directory.\\n\\nreports\\n\\n| — — — attendance.csv\\n\\nmark_attendance.py\\n\\nimport csv\\nclass Mark_Attendance:\\n    def __init__(self,csv_filename):\\n        self.csv_filename = csv_filename\\n    \\n    def write_csv_header(self,staff_name,date,time,status):\\n        self.date = date\\n        self.staff_name = staff_name\\n        self.time = time\\n        self.status = status\\n        f = open(self.csv_filename, \"w+\",newline=\\'\\')\\n        writer = csv.DictWriter(f, fieldnames=[self.staff_name,self.date,self.time,self.status])\\n        writer.writeheader()\\n        f.close()\\n    def append_csv_rows(self,records):\\n        self.records = records\\n        with open(self.csv_filename, \\'a+\\',newline=\\'\\') as f_object: \\n            # Pass this file object to csv.writer() \\n            # and get a writer object \\n            writer_object = csv.writer(f_object) \\n        \\n            # Pass the list as an argument into \\n            # the writerow() \\n            writer_object.writerow(self.records) \\n        \\n            #Close the file object \\n            f_object.close()\\n\\nrecognize.py\\n\\nimport csv\\nclass Mark_Attendance:\\n    def __init__(self,csv_filename):\\n        self.csv_filename = csv_filename\\n    \\n    def write_csv_header(self,staff_name,date,time,status):\\n        self.date = date\\n        self.staff_name = staff_name\\n        self.time = time\\n        self.status = status\\n        f = open(self.csv_filename, \"w+\",newline=\\'\\')\\n        writer = csv.DictWriter(f, fieldnames=[self.staff_name,self.date,self.time,self.status])\\n        writer.writeheader()\\n        f.close()\\n    def append_csv_rows(self,records):\\n        self.records = records\\n        with open(self.csv_filename, \\'a+\\',newline=\\'\\') as f_object: \\n            # Pass this file object to csv.writer() \\n            # and get a writer object \\n            writer_object = csv.writer(f_object) \\n        \\n            # Pass the list as an argument into \\n            # the writerow() \\n            writer_object.writerow(self.records) \\n        \\n            #Close the file object \\n            f_object.close()\\n\\nConclusion\\n\\nI hope you were able to follow along and was able to build the system successfully.\\n\\nTo download the source code to this post,\\xa0Get the code on Github\\n\\nIf you have any questions, recommendations, or critiques, I can be reached via\\xa0LinkedIn. Feel free to reach out to me.\\n\\nFAQ\\n\\nWhat is Face Recognition?\\n\\nFace Recognition\\xa0is a method of identifying an individual by comparing their faces with the stored faces in the database\\n\\nWhat is Face Recognition based attendance system?\\n\\nFace Recognition based attendance system is a system that automatically marks the attendance of present employee by recognizing the face of an employee.\\n\\nWhy do we need Face Recognition based attendance system ?\\n\\nTraditional methods of attendance are\\xa0time and effort consuming. In big organizations where many staff are employed it’s hard to perform attendance manually and there may be chances of misplacement of attendance sheets leading to inaccurate and inefficient attendance. Employees can easily get rid of time recording options by committing\\xa0time theft\\xa0in manual based attendance systems. So, we need Face Recognition based attendance system for not only automating the attendance process but also preventing the employees from commiting time thefts.\\n\\nWhich algorithm is used in the face recognition attendance system?\\n\\nFace recognition is done in a two way process :- Extraction of face embeddings and Training SVM algorithm. We use FaceNet algorithm for extracting 128-d face embeddings. FaceNet maps each face image into 128-d feature vectors such that the the squared distance between two embeddings of the same person is small, whereas the squared distance between two embeddings of different person is large.Then we can train SVM algorithm on those 128-d face embeddings to create a recognizer model.\\n\\nWhere is Face Recognition used ?\\n\\nEveryday, we look into\\xa0our phones thousands of times and end up using face recognition technology for unlocking our phones. Facebook uses it’s own face recognition algorithm for recognizing you and your friends. China has created a vast surveillance system equipped with facial recognition technology to monitor it’s citizens. Similarly, the FBI uses facial recognition to spot criminals.\\n\\nHow FaceId Works ?\\n\\nFace Verification is done in setting up FaceId in smartphones. Imagine you own an Iphone and you want to use Faceid to unlock your phone. First, you need to setup your faceid by capturing your face image at all possible angles. Then the system stores that faces in a database and finally, it verifies you and unlocks your phone by comparing your faces with the stored faces in the database.\\n\\nCategories Artificial Intelligence, featured\\n\\nTags\\n\\nAI,\\n\\nattendance system AI,\\n\\nface recognition system,\\n\\nface recognition system python,\\n\\nPython\\n\\nHow to Get Started on Flutter | For Both Non Programmers and Programmers', metadata={'source': 'https://kickerai.com/face-recognition-attendance-system/'}),\n",
              " Document(page_content='Unwrap Key Failed Flutter Error\\n\\nAugust 20, 2023\\n\\nby Prasant\\n\\nUnwrap Key Failed Error:\\n\\nSolution:\\n\\nAdded the last two lines on AndroidManifest.xml located on android/app/src/main\\n\\nThis solution is given on GitHub. Here’s the link to the solution: 3.1.0: Failed to unwrap key · Issue #13\\n\\nNew Error:\\n\\nIf it doesn’t work and you get Unhandled Exception: PlatformException(Exception encountered, read, javax.crypto.BadPaddingException: error:1e000065:Cipher functions:OPENSSL_internal:BAD_DECRYPT\\n\\nThe solution should be to Clear ALL the data of the App from the Device. Not uninstall but clear the app data.\\n\\nIt will work.\\n\\nNote: Previously, I added the above two lines and then cleared the data. But only clearing the data also did the job.\\n\\nThe reason for this issue is discussed on the Github mentioned above.\\n\\nCategories Flutter\\n\\nTags flutter error and solution, flutter error code, flutter unwrap key failed\\n\\nHow to Get Started on Flutter | For Both Non Programmers and Programmers\\n\\nLeave a Comment\\n\\nComment', metadata={'source': 'https://kickerai.com/unwrap-key-failed-flutter-error/'}),\n",
              " Document(page_content=\"How to Fetch Data from Internet? Flutter Tutorial\\n\\nAugust 20, 2023\\n\\nby Prasant\\n\\nHey Guys, In this article I will teach you how to fetch data from the internet in Flutter.  We will be using the HTTP package to get the data from the API and then get the required data by decoding the response in JSON format.\\n\\nI have also made a Speed coding video showing the process. You can also follow that.\\n\\nTable of Contents:\\n\\nSetting up the Flutter Project\\n\\nFetching the Data from API\\n\\nBuilding the UI\\n\\nNow let’s get started.\\n\\nSetting up the Flutter Project\\n\\nFirst of all, we need to create a new Flutter project. I will be using Vscode in this tutorial.\\n\\nGo to View -> Command Palette -> Flutter: New Application Project\\n\\nNext, Removing all the default codes from the main.dart.\\n\\nAnd adding all the necessary codes, we remain with the following codes in main.dart\\n\\nmain.dart\\n\\nimport 'package:flutter/material.dart';\\nimport 'MyHomePage.dart';\\nvoid main() {\\n  runApp(MyApp());\\n}\\nclass MyApp extends StatelessWidget {\\n  // This widget is the root of your application.\\n  @override\\n  Widget build(BuildContext context) {\\n    return MaterialApp(\\n      theme: ThemeData(\\n        primarySwatch: Colors.blue,\\n      ),\\n      home: MyHomePage(),\\n    );\\n  }\\n}\\n\\nWe will be creating a new file MyHomePage.dart in the same location as main.dart\\n\\nMake a stateful widget MyHomePage in MyHomePage.dart\\n\\nMyHomePage.dart\\n\\nimport 'package:flutter/material.dart';\\nclass MyHomePage extends StatefulWidget {\\n  @override\\n  _MyHomePageState createState() => _MyHomePageState();\\n}\\nclass _MyHomePageState extends State<MyHomePage> {\\n  @override\\n  Widget build(BuildContext context) {\\n    return Container(\\n      \\n    );\\n  }\\n}\\n\\nand Import MyHomePage.dart file and Add MyHomePage() in main.dart\\n\\nThe next step will be to find the API from where we will be fetching the data. For that purpose, I will be using JSONPlaceholder – Free Fake REST API (typicode.com) It’s an open-source REST API that contains useful fake resources like blog posts, comments, photos, users, etc.\\n\\nFrom here we will be getting all the data. Make sure you have installed the JSON Viewer Pro extension on your browser. This extension will be very helpful to understand the structure of the data and to get the path to reach the individual elements in the JSON data.\\n\\nFetching the Data from API\\n\\nTo get the data from the API, we will be using the HTTP package from http | Dart Package (pub.dev)\\n\\nWe will add the HTTP package to pubspec.yaml file as specified in Readme in the site.\\n\\nAfter adding the name of the package, we will do flutter pub get in the terminal or simply CTRL + S.\\n\\nNow we will create an asynchronous function getData() inside MyHomePage() class then we will get the data from the API and store those data into the variable response.\\n\\ngetData() async {\\n    var url = Uri.parse('https://jsonplaceholder.typicode.com/todos/1');\\n    http.Response response = await http.get(url);\\n\\nprint(response.statusCode);\\n\\nWe will print response.StatusCode first to see if we are getting the data from the URL. 200 means we are getting the data but if we get anything but 200, that means we are not receiving the data or there are other errors. That’s why it is important to check first.\\n\\nWe will run the function getData() inside initState. InitState is built first in the runtime before any widgets. So getData() will be called at the very start when the program runs.\\n\\n@override\\n  void initState() {\\n    super.initState();\\n    getData();\\n  }\\n\\nIf we are getting 200, then we will go ahead and use jsonDecode to decode the data from the API which we fetch from response.body\\n\\nFor that purpose, we will import the file which will let us use the decoder.\\n\\nimport\\xa0'dart:convert';\\n\\nWe will be storing the decoded data from response.body to results.\\n\\nNext, we will be declaring different variables to store the values which we will get from the API\\n\\nvar\\xa0userId;\\n\\nvar\\xa0title;\\n\\nvar\\xa0completed;\\n\\nFor example, we will store the decoded value of userId to the declared userId variable.\\n\\nAfter that we will store the value by initializing those variables by using “this.”\\n\\nAll those values will be initialized inside setState(){}. The reason behind this is that the values of those variables will be changing accordingly to the values we will get from the internet.\\n\\nclass _MyHomePageState extends State<MyHomePage> {\\n  \\n  var userId;\\n  var title;\\n  var completed;\\n  getData() async {\\n    var url = Uri.parse('https://jsonplaceholder.typicode.com/todos/1');\\n    http.Response response = await http.get(url);\\n    //print(response.statusCode);\\n    if (response.statusCode == 200) {\\n      var results = jsonDecode(response.body);\\n      setState(() {\\n        this.userId = results['userId'];\\n        this.title = results['title'];\\n        this.completed = results['completed'];\\n      });\\n    } else {\\n      print('error fetching the data');\\n    }\\n  }\\n\\nBe sure that we are getting the data by simply printing a variable as shown in the example.\\n\\nBuilding the UI of the App\\n\\nNext step will be Building the UI of the Application.\\n\\nI have a rough UI in mind which I will be building. You can find the UI making in the video from time 5:33\\n\\nFinal thoughts:\\n\\nI will summarize the whole process of fetching the data from the internet in the points below.\\n\\nAdd the HTTP package from Pub.dev\\n\\nMake a request to API using HTTP package\\n\\nStore the response into a custom variable response.\\n\\nFetch\\xa0and decode the\\xa0data in JSON format\\n\\nDisplay the\\xa0data\\n\\nWith this, We have successfully fetch the data from the internet.\\n\\nYou can find the whole code on Github.\\n\\nIf you have any questions, feel free to ask in the comments below.\\n\\nThanks for reading. Have a good day.\\n\\nFAQ\\n\\nWhat to use to get the data from internet?\\n\\nIf you are using flutter, you can use HTTP Package from Pub.dev site. I have written a detailed blog on how to use the package in your Flutter application. You can check that out for more information.\\n\\nHow do I get data from API?\\n\\nYou can get data from API by using HTTP package in Flutter. I have written a detailed blog on how to use the package in your Flutter application. You can check that out for more information.\\n\\nHow do I display JSON data in flutter?\\n\\nYou can get data from API by using the HTTP package in Flutter. Then use jsonDecode to decode the data from the API. I have written a detailed blog on how to use the package in your Flutter application. You can check that out for more information.\\n\\nIn Flutter, What’s the process of fetching the data from internet?\\n\\nI will summarize the whole process of fetching the data from the internet in the points below.\\n\\n1. Add the HTTP package from Pub.dev\\n\\n2. Make a request to API using HTTP package\\n\\n3. Store the response into a custom Dart object\\n\\nresponse.\\n\\n4. Fetch\\xa0and decode the\\xa0data in JSON format\\n\\n5. Display the\\xa0data\\n\\nWith this, We have successfully fetch the data from the internet.\\n\\nCategories Flutter\\n\\nTags\\n\\nBeginners Flutter Learning,\\n\\nflutter for beginners,\\n\\nget data from internet flutter,\\n\\nhow to fetch data from internet,\\n\\nhow to get started with flutter\\n\\nHow to Get Started on Flutter | For Both Non Programmers and Programmers\", metadata={'source': 'https://kickerai.com/how-to-fetch-data-from-internet-flutter-tutorial/'}),\n",
              " Document(page_content=\"Why isn’t Pubsec in dart?\\n\\nAugust 20, 2023\\n\\nby Prasant\\n\\nHave you thought about why Pubsec isn’t in dart? Why is it in another language? I was curious today, so I did a little bit of searching.\\n\\nBefore we discuss that, Let’s discuss Yaml first. Do you know Yaml stands for Yaml ain’t markup language! It is a programming language and is mostly used because it is human-readable and easy to understand.\\n\\nCodeUtility)\\n\\nIn flutter, we mostly use it for packages and assets(images, fonts, videos, etc).\\n\\nIf you want to know more about it, here’s a good article about Pubsec.Yaml: Pubsec.Yaml Definition and Syntax\\n\\nRelated: Login and Register Easily with Flutter using Firebase\\n\\nNow Let’s discuss Why Pubsec isn’t in dart.\\n\\nI found the answer in this Reddit post.\\n\\n“Hi, I’m one of the co-designers of pub. There are two slightly different ways to ask your question:\\n\\nWhy aren’t pubspecs Dart programs?\\n\\nWe didn’t want pubspecs to contain arbitrary Dart code because we wanted pub and other tools to be able to parse and understand the contents of the file without needing an entire Dart interpreter for complexity reasons. For safety reasons, we didn’t want processing a pubspec to allow running arbitrary code which could read the user’s file system, talk to the network, etc. For performance, we wanted the language to not be Turing-complete so that processing the file would be reliably fast.\\n\\nWhy don’t pubspecs use Dart\\xa0syntax?\\n\\nAs you suggest below, we could have said the file isn’t an entire Dart\\xa0program, but\\xa0does\\xa0use the existing Dart\\xa0map literal syntax. As JSON is to JavaScript, we could have said that the pubspec was “DSON”—a subset of Dart syntax containing only map literals, list literals, etc.\\n\\n(We did consider JSON which Dart can parse already, but it doesn’t support comments. We felt users should be able to comment their pubspecs without having to use hacks like\\xa0'comment': 'some ignored key'.)\\n\\nThis was actually my initial pitch, and I think it would have been a good choice. We talked to the VM team about adding support for parsing a subset of the language since they already had a parser for the full language. At the time (this was a long time ago with a very different team), they didn’t want to do it. That left us writing a custom parser from scratch. I don’t remember the exact reasoning, but we ultimately decided that if we had to write a parser, we should use a language that was already widely established. JSON was out since it didn’t allow comments. XML was too verbose and complex. At the time, YAML was quite popular, so it’s what we went with.\\n\\nIn practice, most pubspecs are pretty simple so it doesn’t matter much.”\\n\\nCheck out other Posts:\\n\\nCreate a Simple Dialog Box in Flutter\\n\\nCreate Games with Flutter | Tic Tac Toe\\n\\nFlutter Error: The Android Gradle plugin supports only kotlin-android-extensions Gradle plugin version 1.6.20 and higher\\n\\nFlutter Errors and Solution\\n\\nFlutter Rounded Container Code Example\\n\\nCategories Flutter\\n\\nTags\\n\\nflutter pubsec,\\n\\nflutter pubsec.yaml,\\n\\npubsec,\\n\\npubsec.yaml\\n\\nHow to Get Started on Flutter | For Both Non Programmers and Programmers\", metadata={'source': 'https://kickerai.com/why-isnt-pubsec-in-dart/'}),\n",
              " Document(page_content='Face Liveness Detection via OpenCV and Tensorflow\\n\\nAugust 20, 2023\\n\\nby Prabhat\\n\\nFace Liveness Detection is a biometric system that can identify whether a  face comes from a real person or a fake person and prevents the system from giving false verification.\\n\\nIf it comes from a real person, then the face recognition system accepts such images and does the recognition process but it rejects the image if it comes from a fake person. Here, the main idea behind face liveness detection is to enhance the security of face recognition systems.\\n\\nTable Of Contents\\n\\nNeed For Face Liveness Detection\\n\\nTypes of Attacks In Face Recognition SystemsPrint Photo AttackMobile Photo AttackReplay AttackPhoto Mask Attack3D Mask Attack\\n\\nSolutions For Face AntiSpoofing SystemEye Blink DetectionChallenge Response System3D CameraDeep Learning Approach\\n\\nInspiration For Building Face Liveness Detection\\n\\nWhere Face Liveness Detection is used in Face Recognition System?\\n\\nHow Face Liveness Detection works?\\n\\nFace Liveness Detection Project Structure\\n\\nSetting Up Our Workspace\\n\\nBuilding the system\\n\\nTesting the system\\n\\nLooking for the source code to this post?\\xa0Get the code on github.\\n\\nNeed For Face Liveness Detection\\n\\nAs Face Recognition System is improving and achieving new heights, attackers are also being smart. They are inventing new techniques to attack the security of such systems. Anyone can trick a face recognition system by using photos, videos, or a different substitute for an authorized person’s face. So, let me clarify this scenario with the help of an example :\\n\\nSuppose you have a face unlock system on your phone, what if someone can access your phone by showing your photos or videos from their phones. This can be too dangerous and someone can access your personal and confidential information without you even knowing it.\\n\\nSo in this post, we are first going to cover the types of attacks  in face recognition systems, then we will look into the solutions to prevent such attacks and finally build a\\xa0face liveness detection method\\xa0that can prevent such attacks.\\n\\nTypes Of Attacks In Face Recognition System\\n\\nSpoofing attacks is still a challenge and a security concern in face recognition systems. Some of the spoofing attacks that can fool a face recognition system security are as follows:\\n\\nPrint Photo Attack : Print Photo Attack is a type of attack in which the attacker uses the printed photo of a person’s face to break the face recognition security.\\n\\n2. Mobile Photo Attack : Mobile Photo Attack is a type of attack in which the attacker tries to attack the system by showing the photos from their mobile phones as a substitue for an authorized person’s face. This type of attack is too common as the digital photo of a person can be easily found in Facebook, Instagram and other social media and the attackers uses such images as a target to bypass the system security.\\n\\n3. Replay Attack : Replay Attack is a type of attack in which the attacker uses the video of a person’s face as a substitue for an authorized person’s face to attack the system.\\n\\n4. Photo Mask Attack : In Photo Mask Attack, the attacker create a mask from a 2d photo and uses that mask to spoof or attack the system.\\n\\n5. 3D Mask Attack : 3D Mask Attack is one of the most sophisticated and expensive attack among all other attacks. In this type of attack, the attacker constructs a\\xa0 3d face mask of a target client and uses it to gain access to the system.\\n\\nHere, we have covered all the types of attack that exists in the Face Recognition System. Now, it’s time to look into the solutions that we can adopt to prevent such kinds of attack. The solution to prevent such kinds of attack is Face Liveness Detection.\\n\\nSolutions For Implementing Face Liveness Detection\\n\\n1.Eye Blink Detection: Eye Blink Detection can be a relevant solution for Mobile Photo Attack and Print Photo attack because the static images are provided as an input to present such kinds of attack. However, human eyes blink around 15 to 20 times in a minute. So, it would be a reliable solution to prevent such kinds of attacks if we can implement eye blink detection in face recognition systems.\\n\\n2. Challenge Response System: In Challenge Response System, one entity sends a challenge to another entity and a second entity must respond with an appropriate answer in order to be authenticated. Some of the challenge response questions that can be implemented in the Face Recognition system are asking a user for smiles, expressions of emotions such as sadness, surprise, head movements, and so on. The user/attacker must respond with an answer to be authenticated.\\n\\n3. 3D Camera: The most reliable anti-spoofing technique uses a 3D camera. If we use 3D camera as an antispoofing solution then we can estimate the depth information of any face image. We can use this depth information to get an estimate of whether it is a real person or not since fake images lack depth value. Precise pixel depth information provides high accuracy against presentation attacks.\\n\\n4. Deep Learning Appraoch : The deep learning architectures (CNN) have proved that they are very good at tackling image classification problems. So, we can also think of the face liveness detection method as a binary classification problem in which we train our CNN to learn feature maps to clearly distinguish between real image and the spoof image. However, deep learning-based solutions will only be strong within the certain datasets, but the network won’t work in real conditions. For making this solution more viable, the network must be trained with millions of images with different environmental conditions and device specifications so that the network can generalize.\\n\\nInspiration For Building Face Liveness Detection\\n\\nMost face recognition systems are not able to distinguish between the real faces and the spoofed ones and as a result the attackers can easily bypass the  system security by showing their pictures to the camera using their phones. So, such systems are not considered reliable enough as they are vulnerable to spoofing attacks.\\n\\nDue to this reason, there is a strong need to upgrade these systems to make them more secure and reliable. So by observing all these things we are inspired to build this project “Face Liveness Detection” which not only improves the existing system but also adapts to some of the security challenges making this system more secure and reliable.\\n\\nHere, we have used the deep learning approach where we have treated the solution as a binary classification problem for identifying ‘real’ or ‘spoof’ faces.\\n\\nWhere Face Liveness Detection Is Used In Face Recognition System ?\\n\\nAfter understanding what face antispoofing system is, questions may arise where Face Antispoofing system comes into picture in Face Recognition Systems. We get an answer to this question by looking at the above figure. Liveness detection is done just before face recognition process.\\n\\nAt first, the staff faces towards the webcam and the webcam captures the portrait face of each staff.\\n\\nThen on those captured portraits, we apply face detection algorithm. After that, the detected face is forwarded to the liveness detection method for checking whether that detected face comes from a live person or not. If it comes from a live person then only the face recognition process is done.\\n\\nHow Face Liveness Detection Works?\\n\\nHere, we are going to understand how face liveness detection works with the help of a flowchart. As we know, we have two possibilities of inputs as well as outputs in face liveness detection. First possibility is a ‘real’ face and second possibility is a ‘fake’ face.\\n\\nThe portrait face of a person is provided as an input to the webcam and  forwarded to the face detection process. Here we use opencv’s haarcascade classifier for detecting the face.  After face detection, the detected face is resized into 160 by 160 pixels.\\n\\nThen those resized faces are provided as an input to the liveness model or the antispoofing model. Finally, the model does the prediction of whether the image comes from a real person or a fake person.\\n\\nIf it comes from a real person, the model predicts the output as ‘real’ and if it come from a fake person, then the model predicts the output as ‘spoof’.\\n\\nFace Liveness Detection Project Structure\\n\\nOur system has two top level directories :\\n\\nantispoofing_models/ :\\xa0contains pretrained model weight file and configuration file for face antispoofing system. ‘antispoofing_model.h5’ is a pretrained model weight file whereas ‘antispoofing_model.json’ is the configuration file of the model.\\n\\nmodels/ :\\xa0contains pretrained model for face detection algorithm. Here OpenCV’s ‘haarcascade_frontalface_default.xml’ is used as a face detection model.\\n\\nWe also have two main file in the root directory:\\n\\nlivelines_net.py : This is the main program file which opens a webcam and loads antispoofing model to classify whether the image in the webcam comes from a real person or spoof.\\n\\nrequirements.txt : This text file contains all the requirements and dependencies file that you need to install before running this system.\\n\\nSetting Up Our WorkSpace\\n\\nFirst, we are going to set up our development environment for this project. We are going to follow some of the steps for setting up our development environment:\\n\\nCreate a virtual environment\\n\\nActivate the virtual environment\\n\\nInstall the dependencies and requirements before running this project. You can check the dependencies and requirements of this project in ‘requirements.txt’ file. There are two main dependencies that needs to be installed for running this system in your development environment. They are OpenCV and Tensorflow. For installing opencv, we use the following command :\\n\\npip install opencv==4.2.0.34\\n\\nFor installing Tensorflow, we use the following command :\\n\\npip install tensorflow==2.4.0\\n\\nBuilding the System\\n\\nNow, we are ready to write some code for building liveness detection method for our project.\\n\\nCreate a new file named ‘livelines_net.py’ file and start to code here.\\n\\nimport cv2\\nfrom tensorflow.keras.preprocessing.image import img_to_array\\nimport os\\nimport numpy as np\\nfrom tensorflow.keras.models import model_from_json\\n\\nLines 1-5\\xa0import our required packages. This script requires OpenCV, Os, Tensorflow and NumPy.\\n\\nroot_dir = os.getcwd()\\n# Load Face Detection Model\\nface_cascade = cv2.CascadeClassifier(\"models/haarcascade_frontalface_default.xml\")\\n# Load Anti-Spoofing Model graph\\njson_file = open(\\'antispoofing_models/antispoofing_model.json\\',\\'r\\')\\nloaded_model_json = json_file.read()\\njson_file.close()\\nmodel = model_from_json(loaded_model_json)\\n# load antispoofing model weights \\nmodel.load_weights(\\'antispoofing_models/antispoofing_model.h5\\')\\nprint(\"Model loaded from disk\")\\n\\nIn lines 6 – 16, the face detection model, pretrained network graph and weights file of an antispoofing model is loaded into the disk for doing prediction in the live webcam data.\\n\\nvideo = cv2.VideoCapture(0)\\nwhile True:\\n    try:\\n        ret,frame = video.read()\\n        gray = cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY)\\n        faces = face_cascade.detectMultiScale(gray,1.3,5)\\n        for (x,y,w,h) in faces:  \\n            face = frame[y-5:y+h+5,x-5:x+w+5]\\n            resized_face = cv2.resize(face,(160,160))\\n            resized_face = resized_face.astype(\"float\") / 255.0\\n            # resized_face = img_to_array(resized_face)\\n            resized_face = np.expand_dims(resized_face, axis=0)\\n            # pass the face ROI through the trained liveness detector\\n            # model to determine if the face is \"real\" or \"fake\"\\n            preds = model.predict(resized_face)[0]\\n            print(preds)\\n            if preds> 0.5:\\n                label = \\'spoof\\'\\n                cv2.putText(frame, label, (x,y - 10),\\n                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,0,255), 2)\\n                cv2.rectangle(frame, (x, y), (x+w,y+h),\\n                    (0, 0, 255), 2)\\n            else:\\n                label = \\'real\\'\\n                cv2.putText(frame, label, (x,y - 10),\\n                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,255,0), 2)\\n                cv2.rectangle(frame, (x, y), (x+w,y+h),\\n                (0, 255, 0), 2)\\n        cv2.imshow(\\'frame\\', frame)\\n        key = cv2.waitKey(1)\\n        if key == ord(\\'q\\'):\\n            break\\n    except Exception as e:\\n        pass\\nvideo.release()        \\ncv2.destroyAllWindows()\\n\\nIn lines 17-24, the webcam is initialized and then the webcam captures the portrait face of a user. On those captured portraits, face detection algorithm is applied to detect and extract the facial region.\\n\\nIn lines 25-52, the extracted face image is resized into 160 by 160 pixels. Then it is forwarded to the antispoofing model for prediction. Then the model does the prediction according to the threshold value. If that value is greater than 0.5 then the predicted class is ‘spoof’ and if the value is lesser than 0.5 then the predicted class is ‘real’. And finally we close our webcam.\\n\\nTesting The System\\n\\nFinally, it’s time to test the system. For testing the system, we have to run our main file which is ‘livelines_net.py’.  For running this file, open terminal in your current working directory and the use the following command :\\n\\npython livelines_net.py\\n\\nAfter running this file, the web camera got initialized and the system can detect ‘real’ if the real face is provided as an input to the system and ‘spoof’ if we try to use photos or videos to spoof the system.\\n\\nConclusion\\n\\nI hope you were able to follow along and was able to build the system successfully.\\n\\nNote :  The pretrained antispoofing model is not the state of the art solution. It has been trained on few datasets and wrk on some environmental condition. So, there is a room for improving the model performance. For improving the accuracy of the model, you have to train the model with large datasets of images.\\n\\nTo download the source code to this post,\\xa0Get the code on github.\\n\\nIf you have any questions, recommendations, or critiques, I can be reached via\\xa0LinkedIn. Feel free to reach out to me.\\n\\nCheck out my Previous Post: Face Recognition Attendance System\\n\\nFAQ\\n\\nWhat is Face Liveness Detection ?\\n\\nFace Liveness Detection is a biometric system which can identify whether a face comes from a real person or a fake person and prevents the system from giving false verification.\\n\\nCan you fake face recognition ?\\n\\nAnyone can fake face recognition system by using photos, videos or a different substitute for an authorized person’s face.\\n\\nHow does face liveness work?\\n\\nLiveness detection model is trained for classifying each face image as ‘real’ or spoof’. So, when webcam is initialized and inputs are provided, the model grabs every frame and checks the probability of that frame belonging to each class and if the predicted probability is higher for ‘real’ class then the model predicts as ‘real’ otherwise ‘spoof’.\\n\\nHow do face recognition systems differentiate between a real face and a photo of a face?\\n\\nFace recognition system must integrate liveness detection method to differentiate between a real face and a photo of a face. If the liveness detection method predicts the face to be real then it is forwarded for the recognition process and rejected otherwise.\\n\\nCategories Artificial Intelligence\\n\\nTags face liveness detection biometrics, face liveness detection opencv python, face liveness detection python\\n\\nHow to Get Started on Flutter | For Both Non Programmers and Programmers\\n\\n2 thoughts on “Face Liveness Detection via OpenCV and Tensorflow”\\n\\nVerdin\\t\\t\\t\\t\\t\\t\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tNovember 14, 2021 at 6:08 am\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\n\\t\\t\\t\\t\\n\\t\\t\\t\\t\\tHello. I’ve been trying to test out the model and decided to train it on my dataset. May I know the directory structure and how you named your images?\\n\\nm riyaz hussain\\t\\t\\t\\t\\t\\t\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tApril 14, 2022 at 8:01 am\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\n\\t\\t\\t\\t\\n\\t\\t\\t\\t\\tcan u share the code\\n\\nLeave a Comment\\n\\nComment', metadata={'source': 'https://kickerai.com/face-liveness-detection-via-opencv-and-tensorflow/'}),\n",
              " Document(page_content='[Solved] Don’t use one refreshController to multiple SmartRefresher\\n\\nAugust 20, 2023\\n\\nby Prasant\\n\\nDon’t use one refreshController to multiple SmartRefresher, It will cause some unexpected bugs mostly in TabBarView\\n\\n‘package:pull_to_refresh/src/smart_refresher.dart’:\\n\\npackage:pull_to_refresh/src/smart_refresher.dart:1\\n\\nFailed assertion: line 608 pos 12: ‘_refresherState == null’\\n\\nThe issue was using one refreshController to multiple SmartRefresher. The solution was quite simple: i.e Define another instance for another smart refresher.\\n\\nHowever, my attention was focused on TabBarView, GridView, Silvers, etc Because many on StackOverflow and other forums mostly discussed those issues focusing on TabBarView.\\n\\nI wasted a lot of time.\\n\\nIssue:\\n\\nI was using two SmartRefresher but one instance of RefreshController. The same goes for the separate _OnRefresh Function.\\n\\nEven When I created two different instances refreshController and _refreshController and passed them to the controller in those two SmartRefresher. _onRefresh function was creating an issue. I was getting Future(dynamic) to void()? …. something like that. I forgot.\\n\\nSolution\\n\\nLike I said earlier, the solution was quite simple which is to pass the different instances of RefreshController to different SmartRefresher.\\n\\nAnother thing, rather than creating a separate function _OnRefresh outside, I directly created the function inside SmartRefresher.\\n\\nThis worked for me. I was using TabBarView, GridView, and Silvers, and even then the code worked fine.\\n\\nIf it doesn’t work out for you you can try the solutions down below:\\n\\nStackoverflow Solution\\n\\nGithub Solution\\n\\nIf you like to know more about Refresher, you can checkout this youtube video: Pull to Refresh and Refresh Indicator Tutorial\\n\\nCategories Flutter\\n\\nTags flutter issue, pull to refresh\\n\\nHow to Get Started on Flutter | For Both Non Programmers and Programmers', metadata={'source': 'https://kickerai.com/solved-dont-use-one-refreshcontroller-to-multiple-smartrefresher/'}),\n",
              " Document(page_content='Create Games with Flutter | Tic Tac Toe\\n\\nAugust 20, 2023\\n\\nby Prasant\\n\\nHello, guys! Do you know you can make games using Flutter?\\n\\nThis week, I learned how to make Tic Tac Toe using Flutter, and in this post, I’ll describe how you can make your own Tic Tac Toe game with Flutter.\\n\\nIt took me two days to design the game itself, then another two days to personalize the app on my own. It was a lot of fun attempting to put this game together.\\n\\nI followed Mitch Koko while creating this application. His YouTube videos are so awesome.\\n\\nCreate Games Using Flutter\\n\\nTable of Contents:\\n\\nIntroductionExplaining the User Interface\\n\\nLogic and CodingCreating a 3 * 3 GridDefining O and XWinning the GameMaking the DrawConclusion and What’s Next\\n\\nIntroduction\\n\\nThis is the intro Screen that the user first sees when he opens the application. I added animation and effects like Hero Animation and Avatar Glow effect.\\n\\nThis is the second screen that users see after the user presses the Play Game Button. There are three expanded widgets.\\n\\nFirst Expanded Widget: This was used for Scoreboard which collects the information on Player O and Player X wins and losses. On the Player’s win, the counter will go from 0 to 1.\\n\\nThird Expanded Widget: This was used for mainly three functions- to reset the counter on Scoreboard, to reset the gameboard and for Day and Night function as shown in the images above.\\n\\nNow Let’s get started.\\n\\nThe Main Logic and Code\\n\\nCreating a 3 * 3 Grid\\n\\nFirst of all, we will be building a 3 by 3 Grid using GridView Builder. The codes are as follows:\\n\\nExpanded(\\n               flex: 3,\\n               child: Container(\\n                 child: GridView.builder(\\n                     itemCount: 9,\\n                     gridDelegate: SliverGridDelegateWithFixedCrossAxisCount(\\n                         crossAxisCount: 3),\\n                     itemBuilder: (BuildContext context, int index) {\\n                       return GestureDetector(\\n                         onTap: () {},\\n                         child: Container(\\n                           child: Center(\\n                             child: Text(\\n                               index,\\n                             ),\\n                           ),\\n                         ),\\n                       );\\n                     }),\\n               ),\\n             ),\\n\\nThe itemcount is set to 9 for 9 boxes. We will be using SliverGridDelegateWithFixedCrossAxisCount with crossAxiscount which we will set to 3, so as to create 3 columns. You can read the documentation of SliverGridDelegateWithFixedCrossAxisCount for more details. We will be returning the GestureDetector.\\n\\nThe GestureDetector requires onTap property.\\n\\nWith onTap, we can define what will happen if users were to click on each box. For now, we will keep the onTap empty.\\n\\nDefining O and X\\n\\nWe first declared oturn as a bool type which can either be true or false only. We will set oturn as true at first.\\n\\nbool\\xa0oturn\\xa0=\\xa0true;\\n\\nList<String>\\xa0display\\xa0=\\xa0[\"\",\\xa0\"\",\\xa0\"\",\\xa0\"\",\\xa0\"\",\\xa0\"\",\\xa0\"\",\\xa0\"\",\\xa0\"\"];\\n\\nWe will also be declaring display as List type which first contains 9 empty items as shown above.\\n\\nNow it’s time to describe the situation when we will use O and X. We will define that in _tapped() method.\\n\\n_tapped(index) {\\n    setState(() {\\n      if (oturn && display[index] == \"\") {\\n        display[index] = \"O\";\\n      } else if (!oturn && display[index] == \"\") {\\n        display[index] = \"X\";\\n      }\\n      oturn = !oturn;\\n      checkwin();\\n    });\\n  }\\n\\nThen with the use of If and else if statements, we will assign X or O in those boxes.\\n\\ndisplay[index] defines the box location where the user tap. For example, display[4]’s location would be the center as shown in the image below.\\n\\nThe first if statement defines if oturn is true and if display[index] is empty then the box will have O when the user taps on the box. Similarly, when oturn is false and if display[index] is empty, then the box will have X.\\n\\nIn the end, we will do ohturn =! ohturn meaning if it is O turn, next it will be X’s turns, but both O and X values/turns cannot be the same at the same time.\\n\\nWe put _tapped(index) method inside onTap.\\n\\nExpanded(\\n               flex: 3,\\n               child: Container(\\n                 child: GridView.builder(\\n                     itemCount: 9,\\n                     gridDelegate: SliverGridDelegateWithFixedCrossAxisCount(\\n                         crossAxisCount: 3),\\n                     itemBuilder: (BuildContext context, int index) {\\n                       return GestureDetector(\\n                         onTap: () {\\n                           _tapped(index);\\n                         },\\n                         child: Container(\\n                           decoration: griddecoration,\\n                           child: Center(\\n                             child: Text(\\n                               display[index],\\n                               style: oxstyle,\\n                             ),\\n                           ),\\n                         ),\\n                       );\\n                     }),\\n               ),\\n             ),\\n\\nWinning the Game\\n\\nWe will also constantly check whether the game has been finished or not with the help of checkwin() method.\\n\\ncheckwin() {\\n    if (display[0] == display[1] &&\\n        display[0] == display[2] &&\\n        display[0] != \"\") {\\n      showDialogBox(display[0]);\\n    }\\n    if (display[3] == display[4] &&\\n        display[3] == display[5] &&\\n        display[3] != \"\") {\\n      showDialogBox(display[3]);\\n    }\\n    if (display[6] == display[7] &&\\n        display[6] == display[8] &&\\n        display[6] != \"\") {\\n      showDialogBox(display[6]);\\n    }\\n    if (display[0] == display[3] &&\\n        display[0] == display[6] &&\\n        display[0] != \"\") {\\n      showDialogBox(display[6]);\\n    }\\n    if (display[1] == display[4] &&\\n        display[1] == display[7] &&\\n        display[1] != \"\") {\\n      showDialogBox(display[1]);\\n    }\\n    if (display[2] == display[5] &&\\n        display[2] == display[8] &&\\n        display[2] != \"\") {\\n      showDialogBox(display[5]);\\n    }\\n    if (display[0] == display[4] &&\\n        display[0] == display[8] &&\\n        display[0] != \"\") {\\n      showDialogBox(display[0]);\\n    }\\n    if (display[2] == display[4] &&\\n        display[2] == display[6] &&\\n        display[2] != \"\") {\\n      showDialogBox(display[6]);\\n    }\\n    if (filledBox == 9) {\\n      showDrawDialog();\\n    }\\n  }\\n\\nSimilarly, we will check the second row 3, 4, 5, and third-row 6, 7, 8. Then next is three columns and we also have to check the two diagonals 0, 4, 8 and 2, 4, 6.\\n\\nMaking the Draw\\n\\nConclusion and What’s Next\\n\\nThat’s all the information needed to build the Tic Tac Toe Game. Now you can also make Tic Tac Toe Game.\\n\\nGood Luck! Make your own Flutter Game.\\n\\nNext Step:\\n\\nI will try to make this Game responsive to most android devices and learn how to put ads on the application. If all done, I will put the TicTacToe on Google Play Store.\\n\\nThanks for reading. Hope you have a good day.\\n\\nCategories Flutter\\n\\nTags flutter for beginners, making tic tac toe using Flutter, tic tac toe with flutter tutorial\\n\\nHow to Get Started on Flutter | For Both Non Programmers and Programmers\\n\\nLeave a Comment\\n\\nComment', metadata={'source': 'https://kickerai.com/create-games-using-flutter-tic-tac-toe/'}),\n",
              " Document(page_content=\"Flutter Error: The Android Gradle plugin supports only kotlin-android-extensions Gradle plugin version 1.6.20 and higher\\n\\nAugust 20, 2023\\n\\nby Prasant\\n\\nThis is a new flutter error I recently came across and wasn’t able to find solutions in Stack Overflow or other forums and sites.\\n\\nlib\\\\main.dart:Launching lib\\\\main.dart on Redmi Note 7 Pro in debug mode...\\n\\nFAILURE: Build failed with an exception.\\n\\nWhat went wrong:The Android Gradle plugin supports only kotlin-android-extensions Gradle plugin version 1.6.20 and higher.\\n\\nThe following dependencies do not satisfy the required version:project ':package_info_plus' -> org.jetbrains.kotlin:kotlin-gradle-plugin:1.6.10*\\n\\nTry:> Run with --stacktrace option to get the stack trace.\\n\\nWhat was the problem? As you can see above, the package info plus package have a Kotlin version 1.6.10, however, the required version for the program to run was 1.6.20 and higher.\\n\\nThe obvious solution would be to update the package. If this doesn’t work, use flutter clean command and clear the cache and again get the package, but this time, changing the versions to latest for all packages got me with this error.\\n\\nAnyway, this solution obviously didn’t work for me, because of conflicts between the packages. If I updated this info plus package, then another package hex color would show this error.\\n\\nSo Instead of updating packages all at ones, I went one by one. If I found the conflict, then I would downgrade to lower versions, until I find a good stable version which gives no conflict error message.\\n\\nI had to do this for 4 packages one by one to find a stable version where there was no conflict between the packages and the Kotlin version was equal or higher than required.\\n\\nThe plugins which can causes conflict can be easily found(in most cases) by using command flutter pub outdated.\\n\\nIf anyone is confused how to use pub outdated command, you can go to the sites below:\\n\\nFind and version bump outdated packages in Flutter\\n\\nDart pub outdated\\n\\nTo further debug the issue, you can go to dependencies folder in your project.\\n\\nCheck out other flutter articles:\\n\\n[Solved] Don’t use one refreshController to multiple SmartRefresher\\n\\nWhy isn’t Pubsec in dart?\\n\\nUnwrap Key Failed Flutter Error\\n\\nLogin and Register Easily with Flutter using Firebase\\n\\nInternet Connection Checker Using Bloc [ Flutter Code Example ]\\n\\nCategories Flutter\\n\\nTags flutter error and solution, kotlin error\\n\\nHow to Get Started on Flutter | For Both Non Programmers and Programmers\", metadata={'source': 'https://kickerai.com/flutter-error-the-android-gradle-plugin-supports-only-kotlin/'})]"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter = CharacterTextSplitter(separator=\"\\n\",\n",
        "                                      chunk_size = 1000,\n",
        "                                      chunk_overlap = 200\n",
        "                                      )\n",
        "# text_splitter = RecursiveCharacterTextSplitter(separators=[\" \", \",\", \"\\n\"],\n",
        "#                                       chunk_size = 1000,\n",
        "#                                       chunk_overlap = 100\n",
        "#                                       )\n"
      ],
      "metadata": {
        "id": "I3gR_Rd6eIEr"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs = text_splitter.split_documents(data)"
      ],
      "metadata": {
        "id": "PlDMkoplmjBX"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fIhkzWg3m3Af",
        "outputId": "fd3b1ac9-4a28-41a3-d23e-058cb37f6f38"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "233"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "docs[10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ULMmd905iLFr",
        "outputId": "e0261fba-2fd6-40b7-f096-ec090eaee415"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(page_content='Flutter is Google’s UI toolkit for building beautiful, natively compiled applications for mobile, web, and desktop from a single codebase.\\nflutter.dev\\nFlutter is easy to learn as well compared to other programming languages.\\nFlutter]\\nSome Other reasons are:\\nYou can create beautiful Apps using Flutter. Designing is easy.\\nReal time compilation. You can change code anytime and easily spot which code is causing problems.\\nYou can save time and money as you don’t need to code differently for IOS or Android.\\nFlutter has a very good documentation, full of rich knowledge about its tutorials, widgets, classes and many more.\\nThese are some of the reasons why Flutter is so popular nowadays.\\n[Note: Flutter is not a programming language, but is a Software Development Kit(SDK) based on Dart.]\\nNow that you know the importance of learning Flutter. Let’s discuss how to get started.', metadata={'source': 'https://kickerai.com/how-to-get-started-on-flutter-for-both-non-programmers-and-programmers/'})"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rQVS733win8S"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LZtcmlMGhT33",
        "outputId": "c2c59dcf-50af-47a8-9d45-2aa6c97da18a"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import faiss\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.embeddings import HuggingFaceHubEmbeddings\n",
        "\n"
      ],
      "metadata": {
        "id": "O9VuCdzsm4ID"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = HuggingFaceHubEmbeddings(huggingfacehub_api_token='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')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ihe4BF7eg_9w",
        "outputId": "df04fd34-8ea5-453f-aaf1-7bcbf3cfb45a"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_deprecation.py:127: FutureWarning: '__init__' (from 'huggingface_hub.inference_api') is deprecated and will be removed from version '1.0'. `InferenceApi` client is deprecated in favor of the more feature-complete `InferenceClient`. Check out this guide to learn how to convert your script to use it: https://huggingface.co/docs/huggingface_hub/guides/inference#legacy-inferenceapi-client.\n",
            "  warnings.warn(warning_message, FutureWarning)\n",
            "You're using a different task than the one specified in the repository. Be sure to know what you're doing :)\n",
            "WARNING:huggingface_hub.inference_api:You're using a different task than the one specified in the repository. Be sure to know what you're doing :)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vectorStore = FAISS.from_documents(docs,embeddings)\n"
      ],
      "metadata": {
        "id": "ewYANqtVhkjf"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# vectorStore = FAISS.from_documents(text,embeddings)\n",
        "\n",
        "# # with open('faiss2.pkl','wb') as f:\n",
        "# #   pickle.dump(vectorStore,f)"
      ],
      "metadata": {
        "id": "kHwxFjdZnZIY"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain import HuggingFaceHub"
      ],
      "metadata": {
        "id": "MJ_m5tj6owZg"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = HuggingFaceHub(repo_id='google/flan-t5-base',huggingfacehub_api_token='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', model_kwargs={\"temperature\":0.9, \"max_length\":512, })"
      ],
      "metadata": {
        "id": "cL5943IJrITR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2cab6de2-00a2-4022-a2bc-7a5cdde9af9c"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_deprecation.py:127: FutureWarning: '__init__' (from 'huggingface_hub.inference_api') is deprecated and will be removed from version '1.0'. `InferenceApi` client is deprecated in favor of the more feature-complete `InferenceClient`. Check out this guide to learn how to convert your script to use it: https://huggingface.co/docs/huggingface_hub/guides/inference#legacy-inferenceapi-client.\n",
            "  warnings.warn(warning_message, FutureWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import RetrievalQAWithSourcesChain"
      ],
      "metadata": {
        "id": "w_tnSs75r5qm"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain = RetrievalQAWithSourcesChain.from_llm(llm = llm, retriever = vectorStore.as_retriever())"
      ],
      "metadata": {
        "id": "mGzIeTabtKVX"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# chain({'question':'What are the Parameters of GridSearchCV'}, return_only_outputs=True) #Temp = 0.1"
      ],
      "metadata": {
        "id": "MY0lquSPtuNs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0fda38ed-23db-47df-ab31-53233407f400"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'answer': 'GridSearchCV: C, gamma, kernel type, n_jobs, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache',\n",
              " 'sources': ''}"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain({'question':'What are the Parameters of GridSearchCV'}, return_only_outputs=True) #Temp = 0.6"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0N09OJPWrwXa",
        "outputId": "0e42c63f-6bf3-496b-b5d8-f5af5a6b9c95"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'answer': 'GridSearchCV: C, gamma, kernel type, n_jobs, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache',\n",
              " 'sources': ''}"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain({'question':'What are the Parameters of GridSearchCV'}, return_only_outputs=True) #Temp = 0.9"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IiZVWAJqsPyo",
        "outputId": "1e011830-1421-4866-f403-bd195eff553c"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'answer': 'GridSearchCV: C, gamma, kernel type, n_jobs, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache',\n",
              " 'sources': ''}"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# chain({'question':'What is Sentiment Analysis'}, return_only_outputs=True)"
      ],
      "metadata": {
        "id": "R4fPdfg6t7Yl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed68951c-b10b-460d-b480-e670d2494fee"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'answer': 'Sentiment analysis is a crucial component of natural language processing (NLP) and has been used to understand customer feedback and social media monitoring.',\n",
              " 'sources': ''}"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain({'question':'What is Sentiment Analysis'}, return_only_outputs=True) #t=0.6"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dtaaIvjhr-18",
        "outputId": "bb59add6-d410-4f9f-e618-e19d92fea331"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'answer': 'Sentiment analysis is a crucial component of natural language processing (NLP) and has been used to understand customer feedback and social media monitoring.',\n",
              " 'sources': ''}"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain({'question':'What are the Parameters of GridSearchCV'}, return_only_outputs=True) #t=0.9"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P7ZuQzhXr1L8",
        "outputId": "87ee3968-957e-480f-84a0-7753227ac15d"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'answer': 'GridSearchCV: C, gamma, kernel type, n_jobs, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache, n_cache',\n",
              " 'sources': ''}"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# chain({'question':'What are the Parameters of GridSearchCV'}, return_only_outputs=True)"
      ],
      "metadata": {
        "id": "6adz73rK0qKa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06679cd1-022c-42ef-8cbb-23403ff3b3a2"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'answer': 'C, gamma, kernel type, rbf', 'sources': ''}"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# chain({'question':'What is Sentiment Analysis'}, return_only_outputs=True)"
      ],
      "metadata": {
        "id": "XQ479VQ_08Om",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "728cc9ab-c72e-4fd1-ab5c-17d25c76dcb9"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'answer': 'Sentiment Analysis is a crucial component of natural language processing (NLP), spanning from understanding customer feedback to social media monitoring.',\n",
              " 'sources': ''}"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain({'question':'How to Perfrom Sentiment Analysis on Nepali Langage?'}, return_only_outputs=True)"
      ],
      "metadata": {
        "id": "PsDKv4s11EqP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6a34424-1e5a-42e2-bcbd-4e9a41ab50a0"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'answer': 'Text: https://kickerai.com/exploring-sentiment-analysis-with-nepali-text/',\n",
              " 'sources': ''}"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# chain({'question':'Provide code to create Rounded Container in Flutter'}, return_only_outputs=True)"
      ],
      "metadata": {
        "id": "qIFqmUqM1NPu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52877c03-675c-4aad-c978-b977835e2c60"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'answer': 'Using the default counter program code.', 'sources': ''}"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain({'question':'Provide code to create Rounded Container in Flutter'}, return_only_outputs=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7StBugnhsGfI",
        "outputId": "ca2c90cc-fe08-4327-f500-66a80f477de4"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'answer': 'Using the default counter program code.', 'sources': ''}"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain({'question':'What is Flutter?'}, return_only_outputs=False)"
      ],
      "metadata": {
        "id": "EygdYZdk1Z7H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27e0a88d-a400-450e-841f-35b0c998ca94"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'question': 'What is Flutter?',\n",
              " 'answer': 'Flutter is Google’s UI toolkit for building beautiful, natively compiled applications for mobile, web, and desktop from a single codebase.',\n",
              " 'sources': ''}"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain({'question':'How to create Rounded Container in Flutter'}, return_only_outputs=True) #t=0.9"
      ],
      "metadata": {
        "id": "1oKMSWLI1wMy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2c9b780-a0ae-44d2-d603-80325cecb209"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'answer': 'Flutter.', 'sources': ''}"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain({'question':'What Flutter Courses Should I read?'}, return_only_outputs=True) #t=0.9"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9dUYwLOasnJu",
        "outputId": "c257917b-ca79-45eb-abc6-80f668f11cf7"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'answer': 'The Complete 2021 Flutter Development Bootcamp with Dart by Dr. Angela Yu',\n",
              " 'sources': ''}"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aSRiPpI_s3NK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}